# -*- coding: utf-8 -*-
"""
SmartEyeSsen Analysis Service (v1.1 - Duplicate Detection Filter Added)
========================================================================

í•™ìŠµì§€ ë¶„ì„ ì„œë¹„ìŠ¤ - ë ˆì´ì•„ì›ƒ ë¶„ì„, OCR, AI ì„¤ëª… ìƒì„±ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
Refactored from api_server.py WorksheetAnalyzer class.

ì£¼ìš” ë³€ê²½ì‚¬í•­ (v1.1):
- analyze_layout: ì¤‘ë³µ íƒì§€(ì˜ˆ: question text vs choices) ì œê±° ìœ„í•œ IoU ê¸°ë°˜ í›„ì²˜ë¦¬ ë¡œì§ ì¶”ê°€.

ì£¼ìš” ë³€ê²½ì‚¬í•­ (v1.0):
- ìƒíƒœ ì €ì¥ ì œê±°: self.layout_info, self.ocr_results, self.api_results ì œê±°
- Mock ëª¨ë¸ ì‚¬ìš©: MockElement, MockTextContent ë°˜í™˜
- ë°˜í™˜ ê°’ ë³€ê²½:
  - analyze_layout() â†’ list[MockElement]
  - perform_ocr() â†’ list[MockTextContent]
  - call_openai_api() â†’ dict[int, str] (element_id â†’ description)
"""

import cv2
import base64
import io
import colorsys
import random
from typing import List, Dict
from PIL import Image
import numpy as np

# AI ë° OCR ê´€ë ¨ íŒ¨í‚¤ì§€
import torch
from huggingface_hub import hf_hub_download
import pytesseract
import openai
from loguru import logger
import platform

# Mock ëª¨ë¸ ì„í¬íŠ¸
from .mock_models import MockElement, MockTextContent, create_mock_element_from_detection, create_mock_text_content

# --- ì‹ ê·œ: ì´ë¯¸ì§€ ì„¤ëª…ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì¶”ê°€ ---
figure_prompt = """
ë‹¹ì‹ ì€ ì‹œê° ì¥ì• ì¸ì„ ìœ„í•œ ì ìë„ì„œ ì‘ì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒ ì´ë¯¸ì§€ë¥¼ ì‹œê° ì¥ì• ì¸ì´ ì™„ì „íˆ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì„¤ëª…í•´ì£¼ì„¸ìš”.

[ì„¤ëª… ê·œì¹™]
1. ì „ì²´ êµ¬ì¡°: ì´ë¯¸ì§€ì˜ ì „ë°˜ì ì¸ í˜•íƒœì™€ êµ¬ì„±ì„ ë¨¼ì € ì„¤ëª…
2. ê³µê°„ ê´€ê³„: ìƒí•˜ì¢Œìš°, ì¤‘ì•™ ë“± ìœ„ì¹˜ ê´€ê³„ë¥¼ ëª…í™•íˆ í‘œí˜„
3. í•µì‹¬ ìš”ì†Œ: ê°€ì¥ ì¤‘ìš”í•œ ì‹œê° ì •ë³´ë¥¼ ìš°ì„ ìˆœìœ„ëŒ€ë¡œ ì„¤ëª…
4. ì„¸ë¶€ ì‚¬í•­: ìƒ‰ìƒ, í¬ê¸°, ëª¨ì–‘ì„ ì´‰ê°ì  í‘œí˜„ìœ¼ë¡œ ë³€í™˜
   - ì˜ˆ: "ì§„í•œ ë¹¨ê°„ìƒ‰" â†’ "ê°•ì¡°ëœ ë¶€ë¶„"
   - ì˜ˆ: "í° ì›" â†’ "ì†ë°”ë‹¥ í¬ê¸°ì˜ ë‘¥ê·¼ í˜•íƒœ"
5. ì˜ë¯¸ ì „ë‹¬: ì´ë¯¸ì§€ê°€ ì „ë‹¬í•˜ë ¤ëŠ” í•µì‹¬ ë©”ì‹œì§€ ìš”ì•½

[ì¶œë ¥ í˜•ì‹]
ì œëª©: [ì´ë¯¸ì§€ì˜ ì£¼ì œ]
êµ¬ì¡°: [ì „ì²´ì ì¸ ë ˆì´ì•„ì›ƒ]
ì£¼ìš” ìš”ì†Œ: [í•µì‹¬ êµ¬ì„±ìš”ì†Œ ë‚˜ì—´]
ìƒì„¸ ì„¤ëª…: [ê° ìš”ì†Œì˜ ê´€ê³„ì™€ ì˜ë¯¸]
í•µì‹¬ ë©”ì‹œì§€: [ì´ë¯¸ì§€ì˜ ëª©ì /ì˜ë„]

[ì˜ˆì‹œ]
ì œëª©: í•œêµ­ì˜ ì¸êµ¬ ì¦ê°€ ê·¸ë˜í”„
êµ¬ì¡°: ê°€ë¡œì¶•ì— ì—°ë„(2000-2025), ì„¸ë¡œì¶•ì— ì¸êµ¬ìˆ˜ê°€ í‘œì‹œëœ ì„  ê·¸ë˜í”„
ì£¼ìš” ìš”ì†Œ: 2000ë…„ 4,700ë§Œì—ì„œ ì‹œì‘í•˜ì—¬ 2025ë…„ 5,200ë§Œê¹Œì§€ ìƒìŠ¹í•˜ëŠ” ê³¡ì„ 
ìƒì„¸ ì„¤ëª…: 2010ë…„ê¹Œì§€ ê¸‰ê²©í•œ ìƒìŠ¹, ì´í›„ ì™„ë§Œí•œ ì¦ê°€ì„¸ë¥¼ ë³´ì„
í•µì‹¬ ë©”ì‹œì§€: í•œêµ­ ì¸êµ¬ëŠ” 25ë…„ê°„ ì•½ 10% ì¦ê°€í–ˆìœ¼ë©°, ìµœê·¼ ì¦ê°€ìœ¨ì´ ë‘”í™”ë¨
"""

table_prompt = """
ë‹¹ì‹ ì€ ì‹œê° ì¥ì• ì¸ì„ ìœ„í•œ ì ìë„ì„œ ì‘ì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒ í‘œë¥¼ ì‹œê° ì¥ì• ì¸ì´ ì ìë¡œ ì½ê¸° ì‰½ë„ë¡ ë³€í™˜í•´ì£¼ì„¸ìš”.

[ë³€í™˜ ê·œì¹™]
1. êµ¬ì¡° ìš°ì„ : í–‰ê³¼ ì—´ì˜ ê°œìˆ˜ë¥¼ ë¨¼ì € ëª…ì‹œ
2. í—¤ë” êµ¬ë¶„: í‘œ ë¨¸ë¦¬ê¸€ì„ ëª…í™•íˆ êµ¬ë¶„í•˜ì—¬ ì œì‹œ
3. ë°ì´í„° ì •ë¦¬: 
   - ìˆ«ìëŠ” ë‹¨ìœ„ì™€ í•¨ê»˜ ëª…ì‹œ
   - ë°±ë¶„ìœ¨ì€ "í¼ì„¼íŠ¸"ë¡œ í‘œê¸°
   - ë¹ˆ ì…€ì€ "ì—†ìŒ"ìœ¼ë¡œ í‘œê¸°
4. ìˆœì°¨ì  ì½ê¸°: ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½, ìœ„ì—ì„œ ì•„ë˜ë¡œ
5. ê´€ê³„ ì„¤ëª…: ë°ì´í„° ê°„ ë¹„êµë‚˜ ì¶”ì„¸ í¬í•¨

[ì¶œë ¥ í˜•ì‹]
í‘œ ì œëª©: [í‘œì˜ ì£¼ì œ]
í‘œ êµ¬ì¡°: [í–‰ ê°œìˆ˜] ê³±í•˜ê¸° [ì—´ ê°œìˆ˜]
ì—´ ì œëª©: [ê° ì—´ì˜ í—¤ë”]
ë°ì´í„°:
  í–‰1: [ë°ì´í„°1], [ë°ì´í„°2], ...
  í–‰2: [ë°ì´í„°1], [ë°ì´í„°2], ...
ì£¼ìš” ë°œê²¬: [í‘œì—ì„œ ì¤‘ìš”í•œ íŒ¨í„´ì´ë‚˜ ì •ë³´]

[ì˜ˆì‹œ]
í‘œ ì œëª©: 2024ë…„ ë¶„ê¸°ë³„ ë§¤ì¶œ ì‹¤ì 
í‘œ êµ¬ì¡°: 5í–‰ ê³±í•˜ê¸° 4ì—´
ì—´ ì œëª©: êµ¬ë¶„, 1ë¶„ê¸°, 2ë¶„ê¸°, 3ë¶„ê¸°
ë°ì´í„°:
  í–‰1(ë§¤ì¶œì•¡): 100ì–µì›, 120ì–µì›, 150ì–µì›
  í–‰2(ì„±ì¥ë¥ ): 10í¼ì„¼íŠ¸, 20í¼ì„¼íŠ¸, 25í¼ì„¼íŠ¸
  í–‰3(ì˜ì—…ì´ìµ): 10ì–µì›, 15ì–µì›, 20ì–µì›
  í–‰4(ìˆœì´ìµ): 8ì–µì›, 12ì–µì›, 18ì–µì›
ì£¼ìš” ë°œê²¬: ë§¤ì¶œì•¡ì´ ë§¤ ë¶„ê¸° ì§€ì†ì ìœ¼ë¡œ ì¦ê°€í•˜ë©°, 3ë¶„ê¸°ì— ê°€ì¥ ë†’ì€ ì„±ì¥ë¥  ê¸°ë¡
"""

flowchart_prompt = """
ë‹¹ì‹ ì€ ì‹œê° ì¥ì• ì¸ì„ ìœ„í•œ ì ìë„ì„œ ì‘ì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒ ìˆœì„œë„ë¥¼ ì‹œê° ì¥ì• ì¸ì´ ë…¼ë¦¬ì  íë¦„ì„ ì™„ì „íˆ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì„¤ëª…í•´ì£¼ì„¸ìš”.

[ì„¤ëª… ê·œì¹™]
1. ì‹œì‘ê³¼ ë: í”„ë¡œì„¸ìŠ¤ì˜ ì‹œì‘ì ê³¼ ì¢…ë£Œì ì„ ëª…í™•íˆ í‘œì‹œ
2. ë‹¨ê³„ë³„ ì„¤ëª…: ê° ë‹¨ê³„ë¥¼ ë²ˆí˜¸ë¥¼ ë§¤ê²¨ ìˆœì°¨ì ìœ¼ë¡œ ì„¤ëª…
3. ë¶„ê¸°ì  ê°•ì¡°: 
   - ê²°ì • í¬ì¸íŠ¸ëŠ” "ë§Œì•½...ë¼ë©´"ìœ¼ë¡œ í‘œí˜„
   - ê° ì„ íƒì§€ì˜ ê²°ê³¼ë¥¼ ëª…ì‹œ
4. ì—°ê²° ê´€ê³„: í™”ì‚´í‘œ ë°©í–¥ì„ "ë‹¤ìŒ", "ì´ë™", "ëŒì•„ê°" ë“±ìœ¼ë¡œ í‘œí˜„
5. ë°˜ë³µ êµ¬ì¡°: ë£¨í”„ë‚˜ ìˆœí™˜ êµ¬ì¡°ë¥¼ ëª…í™•íˆ ì„¤ëª…

[ì¶œë ¥ í˜•ì‹]
ì œëª©: [ìˆœì„œë„ì˜ ëª©ì ]
ì‹œì‘: [ì‹œì‘ ì¡°ê±´ì´ë‚˜ ì…ë ¥]
í”„ë¡œì„¸ìŠ¤:
  ë‹¨ê³„ 1: [ì‘ì—… ë‚´ìš©]
  ë‹¨ê³„ 2: [ì‘ì—… ë‚´ìš©]
  ê²°ì • 1: ë§Œì•½ [ì¡°ê±´]ì´ë¼ë©´
    - ì˜ˆ: [ë‹¤ìŒ ë‹¨ê³„]ë¡œ ì´ë™
    - ì•„ë‹ˆì˜¤: [ë‹¤ë¥¸ ë‹¨ê³„]ë¡œ ì´ë™
  ë‹¨ê³„ 3: [ì‘ì—… ë‚´ìš©]
ì¢…ë£Œ: [ìµœì¢… ê²°ê³¼ë‚˜ ì¶œë ¥]
ì „ì²´ íë¦„ ìš”ì•½: [í”„ë¡œì„¸ìŠ¤ì˜ í•µì‹¬ ëª©ì ]

[ì˜ˆì‹œ]
ì œëª©: ë¡œê·¸ì¸ í”„ë¡œì„¸ìŠ¤
ì‹œì‘: ì‚¬ìš©ìê°€ ë¡œê·¸ì¸ í˜ì´ì§€ ì ‘ì†
í”„ë¡œì„¸ìŠ¤:
  ë‹¨ê³„ 1: ì•„ì´ë””ì™€ ë¹„ë°€ë²ˆí˜¸ ì…ë ¥
  ë‹¨ê³„ 2: ë¡œê·¸ì¸ ë²„íŠ¼ í´ë¦­
  ê²°ì • 1: ë§Œì•½ ì…ë ¥ ì •ë³´ê°€ ì˜¬ë°”ë¥´ë‹¤ë©´
    - ì˜ˆ: ë‹¨ê³„ 3ìœ¼ë¡œ ì´ë™
    - ì•„ë‹ˆì˜¤: ë‹¨ê³„ 4ë¡œ ì´ë™
  ë‹¨ê³„ 3: ë©”ì¸ í˜ì´ì§€ë¡œ ì´ë™í•˜ê³  ì¢…ë£Œ
  ë‹¨ê³„ 4: ì˜¤ë¥˜ ë©”ì‹œì§€ í‘œì‹œ
  ê²°ì • 2: ë§Œì•½ 3íšŒ ì´ìƒ ì‹¤íŒ¨í–ˆë‹¤ë©´
    - ì˜ˆ: ê³„ì • ì ê¸ˆ í›„ ì¢…ë£Œ
    - ì•„ë‹ˆì˜¤: ë‹¨ê³„ 1ë¡œ ëŒì•„ê°
ì¢…ë£Œ: ë¡œê·¸ì¸ ì„±ê³µ ë˜ëŠ” ê³„ì • ì ê¸ˆ
ì „ì²´ íë¦„ ìš”ì•½: ì‚¬ìš©ì ì¸ì¦ì„ í†µí•´ ì‹œìŠ¤í…œ ì ‘ê·¼ ê¶Œí•œì„ ë¶€ì—¬í•˜ëŠ” ë³´ì•ˆ í”„ë¡œì„¸ìŠ¤
"""

# --- ì‹ ê·œ: IoU ê³„ì‚° í•¨ìˆ˜ ì¶”ê°€ ---
def calculate_iou(box1, box2):
    """ë‘ ë°”ìš´ë”© ë°•ìŠ¤ ê°„ì˜ IoU(Intersection over Union) ê³„ì‚°"""
    # box í˜•ì‹: [x1, y1, x2, y2]
    x1_inter = max(box1[0], box2[0])
    y1_inter = max(box1[1], box2[1])
    x2_inter = min(box1[2], box2[2])
    y2_inter = min(box1[3], box2[3])

    inter_area = max(0, x2_inter - x1_inter) * max(0, y2_inter - y1_inter)

    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])
    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])

    union_area = box1_area + box2_area - inter_area

    if union_area == 0:
        return 0.0
    return inter_area / union_area

# --- ì‹ ê·œ: ì¤‘ë³µ ì œê±° í›„ì²˜ë¦¬ í•¨ìˆ˜ ì¶”ê°€ ---
def filter_duplicate_detections(boxes, classes, confs, class_names, iou_threshold=0.7):
    """
    ëª¨ë“  í´ë˜ìŠ¤ ìŒì— ëŒ€í•´ IoU ê¸°ë°˜ìœ¼ë¡œ ì¤‘ë³µ íƒì§€ë¥¼ í•„í„°ë§. (ìë™ ë°©ì‹)
    ì‹ ë¢°ë„ê°€ ë‚®ì€ ìª½ì„ ì œê±°.
    """
    num_detections = len(boxes)
    suppressed = [False] * num_detections # ì œê±°í•  ìš”ì†Œ í‘œì‹œ
    

    indices = list(range(num_detections))
    # ì‹ ë¢°ë„ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬ (ë†’ì€ ê²ƒì„ ë‚¨ê¸°ê¸° ìœ„í•¨)
    indices.sort(key=lambda i: confs[i], reverse=True)

    for i in range(num_detections):
        idx1 = indices[i]
        if suppressed[idx1]:
            continue

        box1 = boxes[idx1]
        # cls_id1 = int(classes[idx1]) # í´ë˜ìŠ¤ ì •ë³´ëŠ” ì œê±° ë¡œì§ì— ë¶ˆí•„ìš”
        # cls_name1 = class_names.get(cls_id1, f"unknown_{cls_id1}") # í´ë˜ìŠ¤ ì •ë³´ëŠ” ì œê±° ë¡œì§ì— ë¶ˆí•„ìš”

        for j in range(i + 1, num_detections):
            idx2 = indices[j]
            if suppressed[idx2]:
                continue

            box2 = boxes[idx2]
            # cls_id2 = int(classes[idx2]) # í´ë˜ìŠ¤ ì •ë³´ëŠ” ì œê±° ë¡œì§ì— ë¶ˆí•„ìš”
            # cls_name2 = class_names.get(cls_id2, f"unknown_{cls_id2}") # í´ë˜ìŠ¤ ì •ë³´ëŠ” ì œê±° ë¡œì§ì— ë¶ˆí•„ìš”
            
            # --- ğŸ‘‡ ìˆ˜ì •ëœ ë¶€ë¶„ ì‹œì‘ ğŸ‘‡ ---
            # íŠ¹ì • í´ë˜ìŠ¤ ìŒ í™•ì¸ ì¡°ê±´ ì œê±°: ëª¨ë“  ìŒì— ëŒ€í•´ IoU ê³„ì‚°
            # if (cls_name1, cls_name2) in problematic_pairs: # ì´ ì¡°ê±´ ì œê±°
            iou = calculate_iou(box1, box2)
            if iou > iou_threshold:
                # ì‹ ë¢°ë„ ë‚®ì€ ìª½(idx2)ì„ ì œê±° ëŒ€ìƒìœ¼ë¡œ í‘œì‹œ
                suppressed[idx2] = True
                # ë¡œê·¸ ë©”ì‹œì§€ì—ì„œ í´ë˜ìŠ¤ ì´ë¦„ ì œê±° (ì„ íƒ ì‚¬í•­)
                logger.debug(f"ì¤‘ë³µ íƒì§€ ì œê±°: Box {idx2}(conf={confs[idx2]:.2f}) - "
                             f"Box {idx1}(conf={confs[idx1]:.2f})ì™€ IoU={iou:.2f} > {iou_threshold}")
            # --- ğŸ‘† ìˆ˜ì •ëœ ë¶€ë¶„ ë ğŸ‘† ---
            
    # ì œê±°ë˜ì§€ ì•Šì€ ìš”ì†Œë“¤ì˜ ì¸ë±ìŠ¤ ë°˜í™˜
    final_indices = [i for i, s in enumerate(suppressed) if not s]
    logger.info(f"ìë™ ì¤‘ë³µ íƒì§€ í•„í„°ë§: {num_detections}ê°œ â†’ {len(final_indices)}ê°œ ìš”ì†Œ (IoU > {iou_threshold})") # ë¡œê·¸ ë©”ì‹œì§€ ìˆ˜ì •
    return final_indices

# Windowsì—ì„œ Tesseract ê²½ë¡œ ì„¤ì • (ê¸°ì¡´ê³¼ ë™ì¼)
if platform.system() == "Windows":
    pytesseract.pytesseract.tesseract_cmd = r'C:\Program Files\Tesseract-OCR\tesseract.exe'

# ë””ë°”ì´ìŠ¤ ì„¤ì • (ê¸°ì¡´ê³¼ ë™ì¼)
device = 'cuda:0' if torch.cuda.is_available() else 'cpu'


class AnalysisService:
    """í•™ìŠµì§€ ë¶„ì„ ì„œë¹„ìŠ¤ - ìƒíƒœ ì—†ëŠ” í•¨ìˆ˜í˜• ë””ìì¸"""

    def __init__(self):
        """ë¶„ì„ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” (ê¸°ì¡´ê³¼ ë™ì¼)"""
        self.model = None
        self.device = device

    def download_model(self, model_choice="SmartEyeSsen"):
        """ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ê¸°ì¡´ê³¼ ë™ì¼)"""
        models = {
            "doclaynet_docsynth": {
                "repo_id": "juliozhao/DocLayout-YOLO-DocLayNet-Docsynth300K_pretrained",
                "filename": "doclayout_yolo_doclaynet_imgsz1120_docsynth_pretrain.pt"
            },
            "docstructbench": {
                "repo_id": "juliozhao/DocLayout-YOLO-DocStructBench",
                "filename": "doclayout_yolo_docstructbench_imgsz1024.pt"
            },
            "docsynth300k": {
                "repo_id": "juliozhao/DocLayout-YOLO-DocSynth300K-pretrain",
                "filename": "doclayout_yolo_docsynth300k_imgsz1600.pt"
            },
            "SmartEyeSsen": {
                "repo_id": "AkJeond/SmartEye",
                "filename": "best.pt"
            }
        }
        selected_model = models.get(model_choice, models["SmartEyeSsen"])
        try:
            logger.info(f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘: {selected_model['repo_id']}")
            filepath = hf_hub_download(
                repo_id=selected_model["repo_id"],
                filename=selected_model["filename"]
            )
            logger.info(f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {filepath}")
            return filepath
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise

    def load_model(self, model_path):
        """ëª¨ë¸ ë¡œë“œ (ê¸°ì¡´ê³¼ ë™ì¼)"""
        try:
            try: from doclayout_yolo import YOLOv10
            except ImportError:
                logger.error("DocLayout-YOLOê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return False
            logger.info("ëª¨ë¸ ë¡œë“œ ì¤‘...")
            self.model = YOLOv10(model_path, task='predict')
            self.model.to(self.device)
            if hasattr(self.model, 'training'): self.model.training = False
            logger.info("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
            return True
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    def analyze_layout(self, image: np.ndarray, model_choice: str = "SmartEyeSsen") -> List[MockElement]:
        """ë ˆì´ì•„ì›ƒ ë¶„ì„ + ì¤‘ë³µ íƒì§€ í•„í„°ë§ ì¶”ê°€"""
        try:
            logger.info("ë ˆì´ì•„ì›ƒ ë¶„ì„ ì‹œì‘...")
            temp_path = "temp_image.jpg"
            cv2.imwrite(temp_path, image)

            if model_choice == "SmartEyeSsen": imgsz, conf = 1024, 0.25
            elif model_choice == "docsynth300k": imgsz, conf = 1600, 0.15
            else: imgsz, conf = 1024, 0.25

            results = self.model.predict(
                temp_path, imgsz=imgsz, conf=conf, iou=0.45, device=self.device
            )

            # --- ê²°ê³¼ ì¶”ì¶œ ---
            boxes = results[0].boxes.xyxy.cpu().numpy() # [x1, y1, x2, y2] í˜•ì‹
            classes = results[0].boxes.cls.cpu().numpy()
            confs = results[0].boxes.conf.cpu().numpy()
            class_names = self.model.names # í´ë˜ìŠ¤ ID -> ì´ë¦„ ë§¤í•‘

            if not boxes.size: # íƒì§€ ê²°ê³¼ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
                logger.warning("ë ˆì´ì•„ì›ƒ ë¶„ì„ ê²°ê³¼, ê°ì§€ëœ ìš”ì†Œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return []

            # --- ì‹ ê·œ: ì¤‘ë³µ íƒì§€ í•„í„°ë§ ---
            final_indices = filter_duplicate_detections(boxes, classes, confs, class_names, iou_threshold=0.7) # IoU 90% ì´ìƒ ê²¹ì¹˜ë©´ ì œê±°

            # --- MockElement ë¦¬ìŠ¤íŠ¸ ìƒì„± (í•„í„°ë§ëœ ì¸ë±ìŠ¤ë§Œ ì‚¬ìš©) ---
            mock_elements = []
            element_id_counter = 1 # ìµœì¢… ìš”ì†Œ IDëŠ” 1ë¶€í„° ì‹œì‘
            for i in final_indices: # í•„í„°ë§ í›„ ë‚¨ì€ ì¸ë±ìŠ¤ë§Œ ì‚¬ìš©
                box = boxes[i]
                cls_id = int(classes[i])
                conf_val = float(confs[i])
                x1, y1, x2, y2 = map(int, box)

                try:
                    cls_name = class_names[cls_id]
                except (IndexError, KeyError): # class_namesê°€ list ë˜ëŠ” dict ì¼ ìˆ˜ ìˆìŒ
                    cls_name = f"unknown_{cls_id}"

                # ë„ˆë¬´ ì‘ì€ ì˜ì—­ ì œì™¸ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
                width = x2 - x1
                height = y2 - y1
                area = width * height
                if area < 100: continue

                # MockElement ìƒì„±
                detection_result = {
                    'class_name': cls_name,
                    'confidence': conf_val,
                    'bbox': [x1, y1, width, height] # [x, y, width, height]
                }
                mock_element = create_mock_element_from_detection(
                    element_id=element_id_counter, # ìˆœì°¨ì  ID ë¶€ì—¬
                    detection_result=detection_result,
                    page_id=None
                )
                mock_elements.append(mock_element)
                element_id_counter += 1

            logger.info(f"ë ˆì´ì•„ì›ƒ ë¶„ì„ ì™„ë£Œ: ìµœì¢… {len(mock_elements)}ê°œ ìš”ì†Œ")
            return mock_elements

        except Exception as e:
            logger.error(f"ë ˆì´ì•„ì›ƒ ë¶„ì„ ì‹¤íŒ¨: {e}", exc_info=True) # ìƒì„¸ ì—ëŸ¬ ë¡œê·¸ ì¶”ê°€
            return []

    def perform_ocr(self, image: np.ndarray, layout_elements: List[MockElement]) -> List[MockTextContent]:
        """OCR ì²˜ë¦¬ (ì˜ì—­ë³„ ì „ì²˜ë¦¬ ì¶”ê°€)"""
        target_classes = [
            'plain text', 'unit', 'question type', 'question text', 'question number', 'title',
            'figure_caption', 'table caption', 'table footnote', 'isolate_formula', 'formula_caption',
            'list', 'choices', 'page', 'second_question_number'
        ]
        ocr_results = []
        custom_config = r'--oem 3 --psm 6'
        logger.info(f"OCR ì²˜ë¦¬ ì‹œì‘... ì´ {len(layout_elements)}ê°œ ë ˆì´ì•„ì›ƒ ìš”ì†Œ ì¤‘ OCR ëŒ€ìƒ í•„í„°ë§")
        logger.info(f"OCR ëŒ€ìƒ í´ë˜ìŠ¤ ëª©ë¡: {target_classes}")
        detected_classes = {elem.class_name for elem in layout_elements} # Setìœ¼ë¡œ ë³€ê²½
        logger.info(f"ê°ì§€ëœ ëª¨ë“  í´ë˜ìŠ¤: {detected_classes}")
        
        target_count = 0
        text_id_counter = 1
        
        for element in layout_elements:
            cls_name = element.class_name # Pydantic ëª¨ë¸ì€ ì´ë¯¸ lower() ë¶ˆí•„ìš”
            logger.debug(f"ë ˆì´ì•„ì›ƒ ID {element.element_id}: í´ë˜ìŠ¤ '{cls_name}' í™•ì¸ ì¤‘...") # DEBUG ë ˆë²¨ë¡œ ë³€ê²½
            if cls_name not in target_classes:
                logger.debug(f"  â†’ OCR ëŒ€ìƒ ì•„ë‹˜")
                continue
            
            target_count += 1
            logger.debug(f"  â†’ OCR ëŒ€ìƒ {target_count}: ID {element.element_id} - í´ë˜ìŠ¤ '{cls_name}'")
            
            # 1. ì˜ì—­ ì´ë¯¸ì§€ ì˜ë¼ë‚´ê¸° (ê¸°ì¡´ ì½”ë“œ)
            x1, y1 = element.bbox_x, element.bbox_y
            x2, y2 = x1 + element.bbox_width, y1 + element.bbox_height
            # ì´ë¯¸ì§€ ê²½ê³„ ë‚´ë¡œ ì¢Œí‘œ ì¡°ì •
            x1, y1 = max(0, x1), max(0, y1)
            x2, y2 = min(image.shape[1], x2), min(image.shape[0], y2)
            
            if y2 <= y1 or x2 <= x1: # í¬ê¸°ê°€ 0ì´ê±°ë‚˜ ìŒìˆ˜ì¸ ê²½ìš° ê±´ë„ˆë›°ê¸°
                logger.warning(f"  â†’ ìœ íš¨í•˜ì§€ ì•Šì€ BBox í¬ê¸°: ID {element.element_id}, ê±´ë„ˆ<0xEB><0x9B><0x84>ëœ€")
                continue
            cropped_img = image[y1:y2, x1:x2]
            
            try:
                # --- ğŸ‘‡ ì˜ì—­ë³„ ì „ì²˜ë¦¬ ë‹¨ê³„ ì‹œì‘ ğŸ‘‡ ---

                # 2. ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜: ìƒ‰ìƒ ì •ë³´ ì œê±°
                gray_img = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2GRAY)

                # 3. ì´ì§„í™” (Otsu's Binarization): í…ìŠ¤íŠ¸/ë°°ê²½ ëª…í™•í™”
                # Otsu ë°©ì‹ì€ ì„ê³„ê°’ì„ ìë™ìœ¼ë¡œ ê²°ì •í•´ ì¤ë‹ˆë‹¤.
                # í•„ìš”ì— ë”°ë¼ cv2.adaptiveThreshold ë“± ë‹¤ë¥¸ ë°©ì‹ ì‚¬ìš© ê°€ëŠ¥
                _, binary_img = cv2.threshold(gray_img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)

                # 4. (ì„ íƒì ) ë…¸ì´ì¦ˆ ì œê±°: Median í•„í„° ì ìš© (ì‘ì€ ì  ì œê±°ì— íš¨ê³¼ì )
                # ì»¤ë„ í¬ê¸°(ì˜ˆ: 3)ëŠ” ì‹¤í—˜ì„ í†µí•´ ì¡°ì •
                denoised_img = cv2.medianBlur(binary_img, 3)

                # --- ğŸ‘† ì˜ì—­ë³„ ì „ì²˜ë¦¬ ë‹¨ê³„ ë ğŸ‘† ---

                # 5. ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ë¡œ OCR ìˆ˜í–‰
                # Pillow ì´ë¯¸ì§€ë¡œ ë³€í™˜ (TesseractëŠ” Pillow ì´ë¯¸ì§€ ì…ë ¥ ì„ í˜¸)
                pil_img = Image.fromarray(cropped_img)
                text = pytesseract.image_to_string(pil_img, lang='kor', config=custom_config).strip()
                
                if len(text) > 1: # ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš°ë§Œ
                    mock_text_content = create_mock_text_content(
                        text_id=text_id_counter, element_id=element.element_id, ocr_result=text,
                        ocr_confidence=None, ocr_engine="Tesseract"
                    )
                    ocr_results.append(mock_text_content)
                    text_id_counter += 1
                    logger.info(f"âœ… OCR ì„±ê³µ: ID {element.element_id} ({cls_name}) - '{text[:50].replace(chr(10), ' ')}...' ({len(text)}ì)") # ê°œí–‰ë¬¸ì ì œê±°
                else: logger.warning(f"âš ï¸ OCR ê²°ê³¼ ì—†ìŒ: ID {element.element_id} ({cls_name})")
            except Exception as e: logger.error(f"OCR ì‹¤íŒ¨: ID {element.element_id} - {e}", exc_info=True) # ìƒì„¸ ì—ëŸ¬
        logger.info(f"OCR ì²˜ë¦¬ ì™„ë£Œ: {len(ocr_results)}ê°œ í…ìŠ¤íŠ¸ ë¸”ë¡")
        return ocr_results


    def call_openai_api(self, image: np.ndarray, layout_elements: List[MockElement], api_key: str) -> Dict[int, str]:
        """OpenAI API í˜¸ì¶œ (ê¸°ì¡´ê³¼ ë™ì¼, ë¡œê¹… ë ˆë²¨ ì¡°ì •)"""
        if not api_key:
            logger.warning("API í‚¤ê°€ ì—†ì–´ AI ì„¤ëª…ì„ ê±´ë„ˆ<0xEB><0x9B><0x84>ëœë‹ˆë‹¤.")
            return {}
        target_classes = ['figure', 'table', 'flowchart']
        ai_descriptions = {}
        try:
            client = openai.OpenAI(api_key=api_key)
            logger.info("OpenAI API ì²˜ë¦¬ ì‹œì‘...")
        except Exception as e:
            logger.error(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return {}
        prompts = {
            'figure': figure_prompt, 
            'table': table_prompt, 
            'flowchart': flowchart_prompt
            }
        system_prompt = "ë‹¹ì‹ ì€ ì‹œê° ì¥ì•  ì•„ë™ í•™ìŠµ AI ë¹„ì„œì…ë‹ˆë‹¤. ì‹œê° ìë£Œ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ê°„ê²°, ëª…í™•í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”. ìŒì„± ë³€í™˜ ê°€ëŠ¥í•˜ê²Œ ì§ì ‘ì ì´ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì‘ì„±í•˜ì„¸ìš”."
        for element in layout_elements:
            cls_name = element.class_name
            if cls_name not in target_classes: continue
            x1, y1 = element.bbox_x, element.bbox_y
            x2, y2 = x1 + element.bbox_width, y1 + element.bbox_height
            if y2 <= y1 or x2 <= x1: continue # í¬ê¸° 0 ë°©ì§€
            cropped_img = image[y1:y2, x1:x2]
            pil_img = Image.fromarray(cv2.cvtColor(cropped_img, cv2.COLOR_BGR2RGB))
            buffered = io.BytesIO(); pil_img.save(buffered, format="PNG")
            img_base64 = base64.b64encode(buffered.getvalue()).decode("utf-8")
            prompt = prompts.get(cls_name, f"ì´ {cls_name} ë‚´ìš© ì„¤ëª…")
            try:
                response = client.chat.completions.create(
                    model="gpt-4-turbo", # ë˜ëŠ” gpt-4o
                    messages=[{"role": "system", "content": system_prompt},
                              {"role": "user", "content": [{"type": "text", "text": prompt},
                                                          {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{img_base64}"}}]}],
                    temperature=0.2, max_tokens=600 )
                description = response.choices[0].message.content.strip()
                ai_descriptions[element.element_id] = description
                logger.info(f"API ì‘ë‹µ ì™„ë£Œ: ID {element.element_id} - {cls_name}")
            except Exception as e: logger.error(f"API ìš”ì²­ ì‹¤íŒ¨: ID {element.element_id} - {e}", exc_info=True) # ìƒì„¸ ì—ëŸ¬
        logger.info(f"OpenAI API ì²˜ë¦¬ ì™„ë£Œ: {len(ai_descriptions)}ê°œ ì„¤ëª… ìƒì„±")
        return ai_descriptions

    def visualize_results(self, image: np.ndarray, layout_elements: List[MockElement]) -> np.ndarray:
        """ê²°ê³¼ ì‹œê°í™” (ê¸°ì¡´ê³¼ ë™ì¼)"""
        img_result = image.copy(); overlay = image.copy()
        random.seed(42)
        unique_classes = list({elem.class_name for elem in layout_elements})
        class_colors = {}
        for i, cls_name in enumerate(unique_classes):
            h, s, v = i / max(1, len(unique_classes)), 0.8, 0.9
            r, g, b = colorsys.hsv_to_rgb(h, s, v)
            class_colors[cls_name] = (int(b * 255), int(g * 255), int(r * 255))
        for element in layout_elements:
            x1, y1 = element.bbox_x, element.bbox_y
            x2, y2 = x1 + element.bbox_width, y1 + element.bbox_height
            cls_name, color = element.class_name, class_colors[element.class_name]
            cv2.rectangle(overlay, (x1, y1), (x2, y2), color, -1)
            cv2.rectangle(img_result, (x1, y1), (x2, y2), color, 2)
            label = f"{cls_name} ({element.confidence:.2f})"
            labelSize, _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)
            y1_label = max(y1, labelSize[1] + 10)
            cv2.rectangle(img_result, (x1, y1_label - labelSize[1] - 10), (x1 + labelSize[0], y1_label), color, -1)
            cv2.putText(img_result, label, (x1, y1_label - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        img_result = cv2.addWeighted(overlay, 0.2, img_result, 0.8, 0)
        return cv2.cvtColor(img_result, cv2.COLOR_BGR2RGB)