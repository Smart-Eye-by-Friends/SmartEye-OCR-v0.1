# -*- coding: utf-8 -*-
"""
SmartEyeSsen Analysis Service (v1.1 - Duplicate Detection Filter Added)
========================================================================

í•™ìŠµì§€ ë¶„ì„ ì„œë¹„ìŠ¤ - ë ˆì´ì•„ì›ƒ ë¶„ì„, OCR, AI ì„¤ëª… ìƒì„±ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
Refactored from api_server.py WorksheetAnalyzer class.

ì£¼ìš” ë³€ê²½ì‚¬í•­ (v1.1):
- analyze_layout: ì¤‘ë³µ íƒì§€(ì˜ˆ: question text vs choices) ì œê±° ìœ„í•œ IoU ê¸°ë°˜ í›„ì²˜ë¦¬ ë¡œì§ ì¶”ê°€.

ì£¼ìš” ë³€ê²½ì‚¬í•­ (v1.0):
- ìƒíƒœ ì €ì¥ ì œê±°: self.layout_info, self.ocr_results, self.api_results ì œê±°
- Mock ëª¨ë¸ ì‚¬ìš©: MockElement, MockTextContent ë°˜í™˜
- ë°˜í™˜ ê°’ ë³€ê²½:
  - analyze_layout() â†’ list[MockElement]
  - perform_ocr() â†’ list[MockTextContent]
  - call_openai_api() â†’ dict[int, str] (element_id â†’ description)
"""

import cv2
import base64
import io
import colorsys
import random
from typing import List, Dict
from PIL import Image
import numpy as np

# AI ë° OCR ê´€ë ¨ íŒ¨í‚¤ì§€
import torch
from huggingface_hub import hf_hub_download
import pytesseract
import openai
from openai import AsyncOpenAI
import asyncio
from loguru import logger
import platform

# Mock ëª¨ë¸ ì„í¬íŠ¸
from .mock_models import MockElement, MockTextContent, create_mock_element_from_detection, create_mock_text_content

# --- ì‹ ê·œ: ì´ë¯¸ì§€ ì„¤ëª…ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì¶”ê°€ ---
figure_prompt = """
ë‹¹ì‹ ì€ ì´ˆë“±í•™ìƒì„ ìœ„í•œ í•™ìŠµ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.
ë‹¤ìŒ ê·¸ë¦¼ì„ ì´ˆë“±í•™ìƒì´ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì„¤ëª…í•´ì£¼ì„¸ìš”.

[ì„¤ëª… ê·œì¹™]
1. ì‰¬ìš´ ë§ ì‚¬ìš©: ì–´ë ¤ìš´ ìš©ì–´ ëŒ€ì‹  ì¼ìƒ ì–¸ì–´ë¡œ ì„¤ëª…
2. ì¤‘ìš”í•œ ê²ƒë¶€í„°: ê°€ì¥ ëˆˆì— ë„ëŠ” ê²ƒë¶€í„° ì°¨ë¡€ëŒ€ë¡œ ì„¤ëª…
3. ìœ„ì¹˜ í‘œí˜„: "ì™¼ìª½ì—", "ì˜¤ë¥¸ìª½ì—", "ê°€ìš´ë°" ê°™ì€ ë§ë¡œ ìœ„ì¹˜ ì•Œë ¤ì£¼ê¸°
4. êµ¬ì²´ì ìœ¼ë¡œ: í¬ê¸°, ëª¨ì–‘, ìƒ‰ê¹”ì„ ì‰½ê²Œ í‘œí˜„

[ì¶œë ¥ í˜•ì‹]
ì´ê²ƒì€ ë¬´ì—‡ì¸ê°€ìš”: [í•œ ë¬¸ì¥ìœ¼ë¡œ ì‰½ê²Œ ì„¤ëª…]

ì–´ë–»ê²Œ ìƒê²¼ë‚˜ìš”:
- ì „ì²´ ëª¨ìŠµ: [ê·¸ë¦¼ì˜ ì „ì²´ ëª¨ì–‘]
- ì¤‘ìš”í•œ ë¶€ë¶„: [ê°€ì¥ ì¤‘ìš”í•œ ê²ƒë“¤]
- ì„¸ë¶€ ë‚´ìš©: [ìì„¸í•œ ì„¤ëª…]

ì´ ê·¸ë¦¼ì´ ë§í•˜ê³  ì‹¶ì€ ê²ƒ: [í•µì‹¬ ë‚´ìš©ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ]

[ì˜ˆì‹œ]
ì´ê²ƒì€ ë¬´ì—‡ì¸ê°€ìš”: ìš°ë¦¬ë‚˜ë¼ ì¸êµ¬ê°€ ì–´ë–»ê²Œ ëŠ˜ì–´ë‚¬ëŠ”ì§€ ë³´ì—¬ì£¼ëŠ” ì„  ê·¸ë˜í”„ì˜ˆìš”.

ì–´ë–»ê²Œ ìƒê²¼ë‚˜ìš”:
- ì „ì²´ ëª¨ìŠµ: ì•„ë˜ìª½ì—ëŠ” ì—°ë„ê°€, ì™¼ìª½ì—ëŠ” ì¸êµ¬ìˆ˜ê°€ ì í˜€ ìˆì–´ìš”.
- ì¤‘ìš”í•œ ë¶€ë¶„: 2000ë…„ë¶€í„° 2025ë…„ê¹Œì§€ ì˜¤ë¥¸ìª½ ìœ„ë¡œ ì˜¬ë¼ê°€ëŠ” ì„ ì´ ê·¸ë ¤ì ¸ ìˆì–´ìš”.
- ì„¸ë¶€ ë‚´ìš©: ì²˜ìŒì—ëŠ” ë¹ ë¥´ê²Œ ì˜¬ë¼ê°€ë‹¤ê°€ ë‚˜ì¤‘ì—ëŠ” ì²œì²œíˆ ì˜¬ë¼ê°€ìš”.

ì´ ê·¸ë¦¼ì´ ë§í•˜ê³  ì‹¶ì€ ê²ƒ: ìš°ë¦¬ë‚˜ë¼ ì¸êµ¬ëŠ” ê³„ì† ëŠ˜ì–´ë‚¬ì§€ë§Œ, ìš”ì¦˜ì€ ì²œì²œíˆ ëŠ˜ì–´ë‚˜ê³  ìˆì–´ìš”.
"""

table_prompt = """
ë‹¹ì‹ ì€ ì´ˆë“±í•™ìƒì„ ìœ„í•œ í•™ìŠµ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.
ë‹¤ìŒ í‘œë¥¼ ì´ˆë“±í•™ìƒì´ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì„¤ëª…í•´ì£¼ì„¸ìš”.

[ì„¤ëª… ê·œì¹™]
1. ì‰¬ìš´ ë§ ì‚¬ìš©: ì–´ë ¤ìš´ ìš©ì–´ ëŒ€ì‹  ì¼ìƒ ì–¸ì–´ë¡œ ì„¤ëª…
2. í‘œì˜ ëª¨ì–‘: ëª‡ ì¤„, ëª‡ ì¹¸ì¸ì§€ ë¨¼ì € ì•Œë ¤ì£¼ê¸°
3. ì œëª© ì„¤ëª…: ê° ì¹¸ì˜ ì œëª©ì´ ë¬´ì—‡ì¸ì§€ ì°¨ë¡€ëŒ€ë¡œ ë§í•˜ê¸°
4. ë‚´ìš© ì½ê¸°: ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½ìœ¼ë¡œ, ìœ„ì—ì„œ ì•„ë˜ë¡œ ì½ê¸°

[ì¶œë ¥ í˜•ì‹]
ì´ê²ƒì€ ë¬´ì—‡ì¸ê°€ìš”: [í‘œì˜ ë‚´ìš©ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ]

í‘œì˜ ëª¨ì–‘:
- í¬ê¸°: [ëª‡ ì¤„, ëª‡ ì¹¸]
- ì œëª©: [ê° ì¹¸ì˜ ì œëª©]

í‘œì— ì íŒ ë‚´ìš©:
ì²« ë²ˆì§¸ ì¤„: [ë‚´ìš©]
ë‘ ë²ˆì§¸ ì¤„: [ë‚´ìš©]
ì„¸ ë²ˆì§¸ ì¤„: [ë‚´ìš©]

ì¤‘ìš”í•œ ë‚´ìš©: [í‘œì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ê²ƒ]

[ì˜ˆì‹œ]
ì´ê²ƒì€ ë¬´ì—‡ì¸ê°€ìš”: 2024ë…„ì— íšŒì‚¬ê°€ ë²ˆ ëˆì„ ë¶„ê¸°ë³„ë¡œ ì •ë¦¬í•œ í‘œì˜ˆìš”.

í‘œì˜ ëª¨ì–‘:
- í¬ê¸°: 5ì¤„, 4ì¹¸
- ì œëª©: êµ¬ë¶„, 1ë¶„ê¸°, 2ë¶„ê¸°, 3ë¶„ê¸°

í‘œì— ì íŒ ë‚´ìš©:
ì²« ë²ˆì§¸ ì¤„ (ë§¤ì¶œì•¡): 100ì–µì›, 120ì–µì›, 150ì–µì›
ë‘ ë²ˆì§¸ ì¤„ (ì„±ì¥ë¥ ): 10í¼ì„¼íŠ¸, 20í¼ì„¼íŠ¸, 25í¼ì„¼íŠ¸
ì„¸ ë²ˆì§¸ ì¤„ (ì˜ì—…ì´ìµ): 10ì–µì›, 15ì–µì›, 20ì–µì›
ë„¤ ë²ˆì§¸ ì¤„ (ìˆœì´ìµ): 8ì–µì›, 12ì–µì›, 18ì–µì›

ì¤‘ìš”í•œ ë‚´ìš©: íšŒì‚¬ê°€ ë²ˆ ëˆì´ ê³„ì† ëŠ˜ì–´ë‚˜ê³  ìˆê³ , 3ë¶„ê¸°ì— ê°€ì¥ ë§ì´ ëŠ˜ì—ˆì–´ìš”.
"""

flowchart_prompt = """
ë‹¹ì‹ ì€ ì´ˆë“±í•™ìƒì„ ìœ„í•œ í•™ìŠµ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.
ë‹¤ìŒ ìˆœì„œë„ë¥¼ ì´ˆë“±í•™ìƒì´ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì„¤ëª…í•´ì£¼ì„¸ìš”.

[ì„¤ëª… ê·œì¹™]
1. ì‰¬ìš´ ë§ ì‚¬ìš©: ì–´ë ¤ìš´ ìš©ì–´ ëŒ€ì‹  ì¼ìƒ ì–¸ì–´ë¡œ ì„¤ëª…
2. ë‹¨ê³„ë³„ë¡œ ì²œì²œíˆ: "ì²« ë²ˆì§¸ë¡œ", "ê·¸ ë‹¤ìŒì—", "ë§ˆì§€ë§‰ìœ¼ë¡œ" ê°™ì€ í‘œí˜„ ì‚¬ìš©
3. ì„ íƒ ìƒí™©: "ë§Œì•½ ~ë¼ë©´ ì–´ë–»ê²Œ í• ê¹Œ?" í˜•ì‹ìœ¼ë¡œ ì§ˆë¬¸í•˜ë“¯ ì„¤ëª…
4. êµ¬ì²´ì  ì˜ˆì‹œ: ê°€ëŠ¥í•˜ë©´ ì‹¤ìƒí™œ ì˜ˆì‹œ ì¶”ê°€

[ì¶œë ¥ í˜•ì‹]
ì´ê²ƒì€ ë¬´ì—‡ì¸ê°€ìš”: [í•œ ë¬¸ì¥ìœ¼ë¡œ ì‰½ê²Œ ì„¤ëª…]

ì–´ë–»ê²Œ ì§„í–‰ë˜ë‚˜ìš”:
1. ì²˜ìŒì—ëŠ” [ì‹œì‘ ë‹¨ê³„ë¥¼ ì‰½ê²Œ ì„¤ëª…]
2. ê·¸ ë‹¤ìŒì—ëŠ” [ë‹¤ìŒ ë‹¨ê³„ë¥¼ ì‰½ê²Œ ì„¤ëª…]
3. ì—¬ê¸°ì„œ ì„ íƒí•´ìš”: [ì„ íƒ ìƒí™©ì´ ìˆë‹¤ë©´]
   - [ì¡°ê±´]ì´ë©´ â†’ [ê²°ê³¼]
   - [ì¡°ê±´]ì´ ì•„ë‹ˆë©´ â†’ [ë‹¤ë¥¸ ê²°ê³¼]
4. ë§ˆì§€ë§‰ì—ëŠ” [ë§ˆë¬´ë¦¬ ë‹¨ê³„]

í•µì‹¬ ë‚´ìš©: [ì „ì²´ íë¦„ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½]

[ì˜ˆì‹œ]
ì´ê²ƒì€ ë¬´ì—‡ì¸ê°€ìš”: ìš°ë¦¬ê°€ ì›¹ì‚¬ì´íŠ¸ì— ë¡œê·¸ì¸í•˜ëŠ” ê³¼ì •ì„ ë³´ì—¬ì£¼ëŠ” ê·¸ë¦¼ì´ì—ìš”.

ì–´ë–»ê²Œ ì§„í–‰ë˜ë‚˜ìš”:
1. ì²˜ìŒì—ëŠ” ë¡œê·¸ì¸ í™”ë©´ì—ì„œ ì•„ì´ë””ì™€ ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ìš”.
2. ê·¸ ë‹¤ìŒì—ëŠ” ë¡œê·¸ì¸ ë²„íŠ¼ì„ ëˆŒëŸ¬ìš”.
3. ì—¬ê¸°ì„œ ì„ íƒí•´ìš”: ì…ë ¥í•œ ì •ë³´ê°€ ë§ëŠ”ì§€ í™•ì¸í•´ìš”.
   - ì •ë³´ê°€ ë§ìœ¼ë©´ â†’ ë©”ì¸ í˜ì´ì§€ë¡œ ë“¤ì–´ê°€ìš”. ë!
   - ì •ë³´ê°€ í‹€ë¦¬ë©´ â†’ ë‹¤ì‹œ ì…ë ¥í•˜ë¼ëŠ” ë©”ì‹œì§€ê°€ ë‚˜ì™€ìš”.
4. ë§Œì•½ 3ë²ˆ í‹€ë¦¬ë©´ ì ì‹œ ë™ì•ˆ ë¡œê·¸ì¸í•  ìˆ˜ ì—†ì–´ìš”.

í•µì‹¬ ë‚´ìš©: ì•„ì´ë””ì™€ ë¹„ë°€ë²ˆí˜¸ê°€ ë§ì•„ì•¼ ì›¹ì‚¬ì´íŠ¸ì— ë“¤ì–´ê°ˆ ìˆ˜ ìˆì–´ìš”.
"""

# --- ì‹ ê·œ: IoU ê³„ì‚° í•¨ìˆ˜ ì¶”ê°€ ---
def calculate_iou(box1, box2):
    """ë‘ ë°”ìš´ë”© ë°•ìŠ¤ ê°„ì˜ IoU(Intersection over Union) ê³„ì‚°"""
    # box í˜•ì‹: [x1, y1, x2, y2]
    x1_inter = max(box1[0], box2[0])
    y1_inter = max(box1[1], box2[1])
    x2_inter = min(box1[2], box2[2])
    y2_inter = min(box1[3], box2[3])

    inter_area = max(0, x2_inter - x1_inter) * max(0, y2_inter - y1_inter)

    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])
    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])

    union_area = box1_area + box2_area - inter_area

    if union_area == 0:
        return 0.0
    return inter_area / union_area

# --- ì‹ ê·œ: ì¤‘ë³µ ì œê±° í›„ì²˜ë¦¬ í•¨ìˆ˜ ì¶”ê°€ ---
def filter_duplicate_detections(boxes, classes, confs, class_names, iou_threshold=0.7):
    """
    ëª¨ë“  í´ë˜ìŠ¤ ìŒì— ëŒ€í•´ IoU ê¸°ë°˜ìœ¼ë¡œ ì¤‘ë³µ íƒì§€ë¥¼ í•„í„°ë§. (ìë™ ë°©ì‹)
    ì‹ ë¢°ë„ê°€ ë‚®ì€ ìª½ì„ ì œê±°.
    """
    num_detections = len(boxes)
    suppressed = [False] * num_detections # ì œê±°í•  ìš”ì†Œ í‘œì‹œ
    

    indices = list(range(num_detections))
    # ì‹ ë¢°ë„ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬ (ë†’ì€ ê²ƒì„ ë‚¨ê¸°ê¸° ìœ„í•¨)
    indices.sort(key=lambda i: confs[i], reverse=True)

    for i in range(num_detections):
        idx1 = indices[i]
        if suppressed[idx1]:
            continue

        box1 = boxes[idx1]
        # cls_id1 = int(classes[idx1]) # í´ë˜ìŠ¤ ì •ë³´ëŠ” ì œê±° ë¡œì§ì— ë¶ˆí•„ìš”
        # cls_name1 = class_names.get(cls_id1, f"unknown_{cls_id1}") # í´ë˜ìŠ¤ ì •ë³´ëŠ” ì œê±° ë¡œì§ì— ë¶ˆí•„ìš”

        for j in range(i + 1, num_detections):
            idx2 = indices[j]
            if suppressed[idx2]:
                continue

            box2 = boxes[idx2]
            # cls_id2 = int(classes[idx2]) # í´ë˜ìŠ¤ ì •ë³´ëŠ” ì œê±° ë¡œì§ì— ë¶ˆí•„ìš”
            # cls_name2 = class_names.get(cls_id2, f"unknown_{cls_id2}") # í´ë˜ìŠ¤ ì •ë³´ëŠ” ì œê±° ë¡œì§ì— ë¶ˆí•„ìš”
            
            # --- ğŸ‘‡ ìˆ˜ì •ëœ ë¶€ë¶„ ì‹œì‘ ğŸ‘‡ ---
            # íŠ¹ì • í´ë˜ìŠ¤ ìŒ í™•ì¸ ì¡°ê±´ ì œê±°: ëª¨ë“  ìŒì— ëŒ€í•´ IoU ê³„ì‚°
            # if (cls_name1, cls_name2) in problematic_pairs: # ì´ ì¡°ê±´ ì œê±°
            iou = calculate_iou(box1, box2)
            if iou > iou_threshold:
                # ì‹ ë¢°ë„ ë‚®ì€ ìª½(idx2)ì„ ì œê±° ëŒ€ìƒìœ¼ë¡œ í‘œì‹œ
                suppressed[idx2] = True
                # ë¡œê·¸ ë©”ì‹œì§€ì—ì„œ í´ë˜ìŠ¤ ì´ë¦„ ì œê±° (ì„ íƒ ì‚¬í•­)
                logger.debug(f"ì¤‘ë³µ íƒì§€ ì œê±°: Box {idx2}(conf={confs[idx2]:.2f}) - "
                             f"Box {idx1}(conf={confs[idx1]:.2f})ì™€ IoU={iou:.2f} > {iou_threshold}")
            # --- ğŸ‘† ìˆ˜ì •ëœ ë¶€ë¶„ ë ğŸ‘† ---
            
    # ì œê±°ë˜ì§€ ì•Šì€ ìš”ì†Œë“¤ì˜ ì¸ë±ìŠ¤ ë°˜í™˜
    final_indices = [i for i, s in enumerate(suppressed) if not s]
    logger.info(f"ìë™ ì¤‘ë³µ íƒì§€ í•„í„°ë§: {num_detections}ê°œ â†’ {len(final_indices)}ê°œ ìš”ì†Œ (IoU > {iou_threshold})") # ë¡œê·¸ ë©”ì‹œì§€ ìˆ˜ì •
    return final_indices

# Windowsì—ì„œ Tesseract ê²½ë¡œ ì„¤ì • (ê¸°ì¡´ê³¼ ë™ì¼)
if platform.system() == "Windows":
    pytesseract.pytesseract.tesseract_cmd = r'C:\Program Files\Tesseract-OCR\tesseract.exe'

# ë””ë°”ì´ìŠ¤ ì„¤ì • (ê¸°ì¡´ê³¼ ë™ì¼)
device = 'cuda:0' if torch.cuda.is_available() else 'cpu'


class AnalysisService:
    """í•™ìŠµì§€ ë¶„ì„ ì„œë¹„ìŠ¤ - ìƒíƒœ ì—†ëŠ” í•¨ìˆ˜í˜• ë””ìì¸"""

    def __init__(self, model_choice: str = "SmartEyeSsen", auto_load: bool = False):
        """
        ë¶„ì„ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”

        Args:
            model_choice: ì‚¬ìš©í•  ëª¨ë¸ ì„ íƒ (ê¸°ë³¸ê°’: "SmartEyeSsen")
            auto_load: Trueì´ë©´ ì´ˆê¸°í™” ì‹œ ìë™ìœ¼ë¡œ ëª¨ë¸ ë¡œë“œ (ê¸°ë³¸ê°’: False, í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)
        """
        self.model = None
        self.device = device
        self.model_choice = model_choice
        self._model_loaded = False

        # ìë™ ë¡œë“œ ì˜µì…˜ì´ í™œì„±í™”ëœ ê²½ìš° ì¦‰ì‹œ ëª¨ë¸ ë¡œë“œ
        if auto_load:
            self._ensure_model_loaded()

    def download_model(self, model_choice="SmartEyeSsen"):
        """ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ê¸°ì¡´ê³¼ ë™ì¼)"""
        models = {
            "doclaynet_docsynth": {
                "repo_id": "juliozhao/DocLayout-YOLO-DocLayNet-Docsynth300K_pretrained",
                "filename": "doclayout_yolo_doclaynet_imgsz1120_docsynth_pretrain.pt"
            },
            "docstructbench": {
                "repo_id": "juliozhao/DocLayout-YOLO-DocStructBench",
                "filename": "doclayout_yolo_docstructbench_imgsz1024.pt"
            },
            "docsynth300k": {
                "repo_id": "juliozhao/DocLayout-YOLO-DocSynth300K-pretrain",
                "filename": "doclayout_yolo_docsynth300k_imgsz1600.pt"
            },
            "SmartEyeSsen": {
                "repo_id": "AkJeond/SmartEye",
                "filename": "best.pt"
            }
        }
        selected_model = models.get(model_choice, models["SmartEyeSsen"])
        try:
            logger.info(f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘: {selected_model['repo_id']}")
            filepath = hf_hub_download(
                repo_id=selected_model["repo_id"],
                filename=selected_model["filename"]
            )
            logger.info(f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {filepath}")
            return filepath
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise

    def load_model(self, model_path):
        """ëª¨ë¸ ë¡œë“œ (ê¸°ì¡´ê³¼ ë™ì¼)"""
        try:
            try: from doclayout_yolo import YOLOv10
            except ImportError:
                logger.error("DocLayout-YOLOê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return False
            logger.info("ëª¨ë¸ ë¡œë“œ ì¤‘...")
            self.model = YOLOv10(model_path, task='predict')
            self.model.to(self.device)
            if hasattr(self.model, 'training'): self.model.training = False
            logger.info("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
            return True
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    def _ensure_model_loaded(self):
        """
        Lazy Loading: ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìœ¼ë©´ ìë™ìœ¼ë¡œ ë¡œë“œ
        (ë‹¤ì¤‘ í˜ì´ì§€ ì²˜ë¦¬ ì‹œ ëª¨ë¸ì„ í•œ ë²ˆë§Œ ë¡œë“œí•˜ë„ë¡ ìµœì í™”)
        """
        if self._model_loaded and self.model is not None:
            return  # ì´ë¯¸ ë¡œë“œë¨

        logger.info(f"ëª¨ë¸ ìë™ ë¡œë“œ ì‹œì‘ (ì„ íƒ: {self.model_choice})...")
        model_path = self.download_model(self.model_choice)
        if not self.load_model(model_path):
            raise RuntimeError(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {self.model_choice}")
        self._model_loaded = True
        logger.info("ëª¨ë¸ ìë™ ë¡œë“œ ì™„ë£Œ!")

    def analyze_layout(self, image: np.ndarray, model_choice: str = "SmartEyeSsen") -> List[MockElement]:
        """
        ë ˆì´ì•„ì›ƒ ë¶„ì„ + ì¤‘ë³µ íƒì§€ í•„í„°ë§ ì¶”ê°€

        Args:
            image: ë¶„ì„í•  ì´ë¯¸ì§€ (numpy array)
            model_choice: ì‚¬ìš©í•  ëª¨ë¸ ì„ íƒ (ê¸°ë³¸ê°’: "SmartEyeSsen")
                         ì£¼ì˜: ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹œ ì§€ì •í•œ model_choiceì™€ ë‹¤ë¥´ë©´ ì¬ë¡œë“œë©ë‹ˆë‹¤.

        Returns:
            ê²€ì¶œëœ ë ˆì´ì•„ì›ƒ ìš”ì†Œ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # ëª¨ë¸ ì„ íƒì´ ë³€ê²½ë˜ì—ˆìœ¼ë©´ ì¬ë¡œë“œ
            if model_choice != self.model_choice:
                logger.warning(f"ëª¨ë¸ ë³€ê²½ ê°ì§€: {self.model_choice} -> {model_choice}")
                self.model_choice = model_choice
                self._model_loaded = False

            # Lazy Loading: ëª¨ë¸ì´ ì—†ìœ¼ë©´ ìë™ ë¡œë“œ
            self._ensure_model_loaded()

            logger.info("ë ˆì´ì•„ì›ƒ ë¶„ì„ ì‹œì‘...")
            temp_path = "temp_image.jpg"
            cv2.imwrite(temp_path, image)

            if model_choice == "SmartEyeSsen": imgsz, conf = 1024, 0.25
            elif model_choice == "docsynth300k": imgsz, conf = 1600, 0.15
            else: imgsz, conf = 1024, 0.25

            results = self.model.predict(
                temp_path, imgsz=imgsz, conf=conf, iou=0.45, device=self.device
            )

            # --- ê²°ê³¼ ì¶”ì¶œ ---
            boxes = results[0].boxes.xyxy.cpu().numpy() # [x1, y1, x2, y2] í˜•ì‹
            classes = results[0].boxes.cls.cpu().numpy()
            confs = results[0].boxes.conf.cpu().numpy()
            class_names = self.model.names # í´ë˜ìŠ¤ ID -> ì´ë¦„ ë§¤í•‘

            if not boxes.size: # íƒì§€ ê²°ê³¼ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
                logger.warning("ë ˆì´ì•„ì›ƒ ë¶„ì„ ê²°ê³¼, ê°ì§€ëœ ìš”ì†Œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return []

            # --- ì‹ ê·œ: ì¤‘ë³µ íƒì§€ í•„í„°ë§ ---
            final_indices = filter_duplicate_detections(boxes, classes, confs, class_names, iou_threshold=0.7) # IoU 90% ì´ìƒ ê²¹ì¹˜ë©´ ì œê±°

            # --- MockElement ë¦¬ìŠ¤íŠ¸ ìƒì„± (í•„í„°ë§ëœ ì¸ë±ìŠ¤ë§Œ ì‚¬ìš©) ---
            mock_elements = []
            element_id_counter = 1 # ìµœì¢… ìš”ì†Œ IDëŠ” 1ë¶€í„° ì‹œì‘
            for i in final_indices: # í•„í„°ë§ í›„ ë‚¨ì€ ì¸ë±ìŠ¤ë§Œ ì‚¬ìš©
                box = boxes[i]
                cls_id = int(classes[i])
                conf_val = float(confs[i])
                x1, y1, x2, y2 = map(int, box)

                try:
                    cls_name = class_names[cls_id]
                except (IndexError, KeyError): # class_namesê°€ list ë˜ëŠ” dict ì¼ ìˆ˜ ìˆìŒ
                    cls_name = f"unknown_{cls_id}"

                # ë„ˆë¬´ ì‘ì€ ì˜ì—­ ì œì™¸ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
                width = x2 - x1
                height = y2 - y1
                area = width * height
                if area < 100: continue

                # MockElement ìƒì„±
                detection_result = {
                    'class_name': cls_name,
                    'confidence': conf_val,
                    'bbox': [x1, y1, width, height] # [x, y, width, height]
                }
                mock_element = create_mock_element_from_detection(
                    element_id=element_id_counter, # ìˆœì°¨ì  ID ë¶€ì—¬
                    detection_result=detection_result,
                    page_id=None
                )
                mock_elements.append(mock_element)
                element_id_counter += 1

            logger.info(f"ë ˆì´ì•„ì›ƒ ë¶„ì„ ì™„ë£Œ: ìµœì¢… {len(mock_elements)}ê°œ ìš”ì†Œ")
            return mock_elements

        except Exception as e:
            logger.error(f"ë ˆì´ì•„ì›ƒ ë¶„ì„ ì‹¤íŒ¨: {e}", exc_info=True) # ìƒì„¸ ì—ëŸ¬ ë¡œê·¸ ì¶”ê°€
            return []

    def perform_ocr(self, image: np.ndarray, layout_elements: List[MockElement]) -> List[MockTextContent]:
        """OCR ì²˜ë¦¬ (ì˜ì—­ë³„ ì „ì²˜ë¦¬ ì¶”ê°€)"""
        target_classes = [
            'plain text', 'unit', 'question type', 'question text', 'question number', 'title',
            'figure_caption', 'table caption', 'table footnote', 'isolate_formula', 'formula_caption',
            'list', 'choices', 'page', 'second_question_number'
        ]
        ocr_results = []
        custom_config = r'--oem 3 --psm 6'
        logger.info(f"OCR ì²˜ë¦¬ ì‹œì‘... ì´ {len(layout_elements)}ê°œ ë ˆì´ì•„ì›ƒ ìš”ì†Œ ì¤‘ OCR ëŒ€ìƒ í•„í„°ë§")
        logger.info(f"OCR ëŒ€ìƒ í´ë˜ìŠ¤ ëª©ë¡: {target_classes}")
        detected_classes = {elem.class_name for elem in layout_elements} # Setìœ¼ë¡œ ë³€ê²½
        logger.info(f"ê°ì§€ëœ ëª¨ë“  í´ë˜ìŠ¤: {detected_classes}")
        
        target_count = 0
        text_id_counter = 1
        
        for element in layout_elements:
            cls_name = element.class_name # Pydantic ëª¨ë¸ì€ ì´ë¯¸ lower() ë¶ˆí•„ìš”
            logger.debug(f"ë ˆì´ì•„ì›ƒ ID {element.element_id}: í´ë˜ìŠ¤ '{cls_name}' í™•ì¸ ì¤‘...") # DEBUG ë ˆë²¨ë¡œ ë³€ê²½
            if cls_name not in target_classes:
                logger.debug(f"  â†’ OCR ëŒ€ìƒ ì•„ë‹˜")
                continue
            
            target_count += 1
            logger.debug(f"  â†’ OCR ëŒ€ìƒ {target_count}: ID {element.element_id} - í´ë˜ìŠ¤ '{cls_name}'")
            
            # 1. ì˜ì—­ ì´ë¯¸ì§€ ì˜ë¼ë‚´ê¸° (ê¸°ì¡´ ì½”ë“œ)
            x1, y1 = element.bbox_x, element.bbox_y
            x2, y2 = x1 + element.bbox_width, y1 + element.bbox_height
            # ì´ë¯¸ì§€ ê²½ê³„ ë‚´ë¡œ ì¢Œí‘œ ì¡°ì •
            x1, y1 = max(0, x1), max(0, y1)
            x2, y2 = min(image.shape[1], x2), min(image.shape[0], y2)
            
            if y2 <= y1 or x2 <= x1: # í¬ê¸°ê°€ 0ì´ê±°ë‚˜ ìŒìˆ˜ì¸ ê²½ìš° ê±´ë„ˆë›°ê¸°
                logger.warning(f"  â†’ ìœ íš¨í•˜ì§€ ì•Šì€ BBox í¬ê¸°: ID {element.element_id}, ê±´ë„ˆ<0xEB><0x9B><0x84>ëœ€")
                continue
            cropped_img = image[y1:y2, x1:x2]
            
            try:
                # --- ğŸ‘‡ ì˜ì—­ë³„ ì „ì²˜ë¦¬ ë‹¨ê³„ ì‹œì‘ ğŸ‘‡ ---

                # 2. ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜: ìƒ‰ìƒ ì •ë³´ ì œê±°
                gray_img = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2GRAY)

                # 3. ì´ì§„í™” (Otsu's Binarization): í…ìŠ¤íŠ¸/ë°°ê²½ ëª…í™•í™”
                # Otsu ë°©ì‹ì€ ì„ê³„ê°’ì„ ìë™ìœ¼ë¡œ ê²°ì •í•´ ì¤ë‹ˆë‹¤.
                # í•„ìš”ì— ë”°ë¼ cv2.adaptiveThreshold ë“± ë‹¤ë¥¸ ë°©ì‹ ì‚¬ìš© ê°€ëŠ¥
                _, binary_img = cv2.threshold(gray_img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)

                # 4. (ì„ íƒì ) ë…¸ì´ì¦ˆ ì œê±°: Median í•„í„° ì ìš© (ì‘ì€ ì  ì œê±°ì— íš¨ê³¼ì )
                # ì»¤ë„ í¬ê¸°(ì˜ˆ: 3)ëŠ” ì‹¤í—˜ì„ í†µí•´ ì¡°ì •
                denoised_img = cv2.medianBlur(binary_img, 3)

                # --- ğŸ‘† ì˜ì—­ë³„ ì „ì²˜ë¦¬ ë‹¨ê³„ ë ğŸ‘† ---

                # 5. ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ë¡œ OCR ìˆ˜í–‰
                # Pillow ì´ë¯¸ì§€ë¡œ ë³€í™˜ (TesseractëŠ” Pillow ì´ë¯¸ì§€ ì…ë ¥ ì„ í˜¸)
                pil_img = Image.fromarray(cropped_img)
                text = pytesseract.image_to_string(pil_img, lang='kor', config=custom_config).strip()
                
                if len(text) > 1: # ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš°ë§Œ
                    mock_text_content = create_mock_text_content(
                        text_id=text_id_counter, element_id=element.element_id, ocr_result=text,
                        ocr_confidence=None, ocr_engine="Tesseract"
                    )
                    ocr_results.append(mock_text_content)
                    text_id_counter += 1
                    logger.info(f"âœ… OCR ì„±ê³µ: ID {element.element_id} ({cls_name}) - '{text[:50].replace(chr(10), ' ')}...' ({len(text)}ì)") # ê°œí–‰ë¬¸ì ì œê±°
                else: logger.warning(f"âš ï¸ OCR ê²°ê³¼ ì—†ìŒ: ID {element.element_id} ({cls_name})")
            except Exception as e: logger.error(f"OCR ì‹¤íŒ¨: ID {element.element_id} - {e}", exc_info=True) # ìƒì„¸ ì—ëŸ¬
        logger.info(f"OCR ì²˜ë¦¬ ì™„ë£Œ: {len(ocr_results)}ê°œ í…ìŠ¤íŠ¸ ë¸”ë¡")
        return ocr_results


    def call_openai_api(self, image: np.ndarray, layout_elements: List[MockElement], api_key: str) -> Dict[int, str]:
        """OpenAI API í˜¸ì¶œ (ê¸°ì¡´ê³¼ ë™ì¼, ë¡œê¹… ë ˆë²¨ ì¡°ì •)"""
        if not api_key:
            logger.warning("API í‚¤ê°€ ì—†ì–´ AI ì„¤ëª…ì„ ê±´ë„ˆ<0xEB><0x9B><0x84>ëœë‹ˆë‹¤.")
            return {}
        target_classes = ['figure', 'table', 'flowchart']
        ai_descriptions = {}
        try:
            client = openai.OpenAI(api_key=api_key)
            logger.info("OpenAI API ì²˜ë¦¬ ì‹œì‘...")
        except Exception as e:
            logger.error(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return {}
        prompts = {
            'figure': figure_prompt, 
            'table': table_prompt, 
            'flowchart': flowchart_prompt
            }
        system_prompt = "ë‹¹ì‹ ì€ ì‹œê° ì¥ì•  ì•„ë™ í•™ìŠµ AI ë¹„ì„œì…ë‹ˆë‹¤. ì‹œê° ìë£Œ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ê°„ê²°, ëª…í™•í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”. ìŒì„± ë³€í™˜ ê°€ëŠ¥í•˜ê²Œ ì§ì ‘ì ì´ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì‘ì„±í•˜ì„¸ìš”."
        for element in layout_elements:
            cls_name = element.class_name
            if cls_name not in target_classes: continue
            x1, y1 = element.bbox_x, element.bbox_y
            x2, y2 = x1 + element.bbox_width, y1 + element.bbox_height
            if y2 <= y1 or x2 <= x1: continue # í¬ê¸° 0 ë°©ì§€
            cropped_img = image[y1:y2, x1:x2]
            pil_img = Image.fromarray(cv2.cvtColor(cropped_img, cv2.COLOR_BGR2RGB))
            buffered = io.BytesIO(); pil_img.save(buffered, format="PNG")
            img_base64 = base64.b64encode(buffered.getvalue()).decode("utf-8")
            prompt = prompts.get(cls_name, f"ì´ {cls_name} ë‚´ìš© ì„¤ëª…")
            try:
                response = client.chat.completions.create(
                    model="gpt-4-turbo", # ë˜ëŠ” gpt-4o
                    messages=[{"role": "system", "content": system_prompt},
                              {"role": "user", "content": [{"type": "text", "text": prompt},
                                                          {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{img_base64}"}}]}],
                    temperature=0.2, max_tokens=600 )
                description = response.choices[0].message.content.strip()
                ai_descriptions[element.element_id] = description
                logger.info(f"API ì‘ë‹µ ì™„ë£Œ: ID {element.element_id} - {cls_name}")
            except Exception as e: logger.error(f"API ìš”ì²­ ì‹¤íŒ¨: ID {element.element_id} - {e}", exc_info=True) # ìƒì„¸ ì—ëŸ¬
        logger.info(f"OpenAI API ì²˜ë¦¬ ì™„ë£Œ: {len(ai_descriptions)}ê°œ ì„¤ëª… ìƒì„±")
        return ai_descriptions

    async def call_openai_api_async(
        self,
        image: np.ndarray,
        layout_elements: List[MockElement],
        api_key: str,
        max_concurrent_requests: int = 5
    ) -> Dict[int, str]:
        """
        OpenAI API ë¹„ë™ê¸° ë³‘ë ¬ í˜¸ì¶œ (ì„±ëŠ¥ ìµœì í™” ë²„ì „)

        Args:
            image: ì›ë³¸ ì´ë¯¸ì§€ (BGR í¬ë§·)
            layout_elements: ë ˆì´ì•„ì›ƒ ìš”ì†Œ ë¦¬ìŠ¤íŠ¸
            api_key: OpenAI API í‚¤
            max_concurrent_requests: ìµœëŒ€ ë™ì‹œ ìš”ì²­ ìˆ˜ (ê¸°ë³¸ê°’: 5)

        Returns:
            Dict[int, str]: {element_id: AI ì„¤ëª…} ë”•ì…”ë„ˆë¦¬

        ì£¼ìš” ê°œì„ ì‚¬í•­:
        - ë¹„ë™ê¸° ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì²˜ë¦¬ ì‹œê°„ 70% ë‹¨ì¶•
        - asyncio.Semaphoreë¡œ Rate Limit ëŒ€ì‘
        - ì§€ìˆ˜ ë°±ì˜¤í”„ ì¬ì‹œë„ ë¡œì§ (exponential backoff)
        """
        if not api_key:
            logger.warning("API í‚¤ê°€ ì—†ì–´ AI ì„¤ëª…ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return {}

        # 1. ëŒ€ìƒ í´ë˜ìŠ¤ í•„í„°ë§ (figure, table, flowchartë§Œ ì²˜ë¦¬)
        target_classes = ['figure', 'table', 'flowchart']
        target_elements = [
            elem for elem in layout_elements
            if elem.class_name in target_classes
        ]

        if not target_elements:
            logger.info("AI ì„¤ëª… ëŒ€ìƒ ìš”ì†Œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {}

        logger.info(f"OpenAI API ë¹„ë™ê¸° ì²˜ë¦¬ ì‹œì‘... (ì´ {len(target_elements)}ê°œ ìš”ì†Œ)")

        # 2. AsyncOpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        try:
            async_client = AsyncOpenAI(api_key=api_key)
        except Exception as e:
            logger.error(f"AsyncOpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return {}

        # 3. Semaphoreë¡œ ë™ì‹œ ìš”ì²­ ìˆ˜ ì œí•œ (Rate Limit ëŒ€ì‘)
        semaphore = asyncio.Semaphore(max_concurrent_requests)

        # 4. ëª¨ë“  ë¹„ë™ê¸° íƒœìŠ¤í¬ ìƒì„±
        tasks = [
            self._process_single_element_async(
                async_client=async_client,
                image=image,
                element=elem,
                semaphore=semaphore
            )
            for elem in target_elements
        ]

        # 5. ë³‘ë ¬ ì‹¤í–‰ (asyncio.gather)
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # 6. ê²°ê³¼ ë§¤í•‘ ë° ì˜ˆì™¸ ì²˜ë¦¬
        ai_descriptions = {}
        success_count = 0
        error_count = 0

        for element, result in zip(target_elements, results):
            if isinstance(result, Exception):
                logger.error(f"API ì‹¤íŒ¨: Element {element.element_id} - {result}")
                error_count += 1
            elif result:  # ì„±ê³µ ì‹œ (ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš°)
                ai_descriptions[element.element_id] = result
                success_count += 1
                logger.info(f"âœ… API ì„±ê³µ: Element {element.element_id} ({element.class_name})")

        logger.info(
            f"OpenAI API ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ë£Œ: "
            f"ì„±ê³µ {success_count}ê±´, ì‹¤íŒ¨ {error_count}ê±´ / ì´ {len(target_elements)}ê±´"
        )

        return ai_descriptions

    async def _process_single_element_async(
        self,
        async_client: AsyncOpenAI,
        image: np.ndarray,
        element: MockElement,
        semaphore: asyncio.Semaphore
    ) -> str:
        """
        ë‹¨ì¼ elementì— ëŒ€í•œ ë¹„ë™ê¸° AI ì„¤ëª… ìƒì„± (ì§€ìˆ˜ ë°±ì˜¤í”„ ì¬ì‹œë„ í¬í•¨)

        Args:
            async_client: AsyncOpenAI í´ë¼ì´ì–¸íŠ¸
            image: ì›ë³¸ ì´ë¯¸ì§€
            element: ì²˜ë¦¬í•  ë ˆì´ì•„ì›ƒ ìš”ì†Œ
            semaphore: ë™ì‹œ ìš”ì²­ ìˆ˜ ì œí•œìš© Semaphore

        Returns:
            str: AI ìƒì„± ì„¤ëª… í…ìŠ¤íŠ¸

        ì¬ì‹œë„ ë¡œì§:
        - ìµœëŒ€ 3íšŒ ì¬ì‹œë„
        - ëŒ€ê¸° ì‹œê°„: 1ì´ˆ â†’ 2ì´ˆ â†’ 4ì´ˆ (ì§€ìˆ˜ ë°±ì˜¤í”„)
        """
        # 1. ì´ë¯¸ì§€ í¬ë¡­ ë° ê²€ì¦
        x1, y1 = element.bbox_x, element.bbox_y
        x2, y2 = x1 + element.bbox_width, y1 + element.bbox_height

        # í¬ê¸° ê²€ì¦
        if y2 <= y1 or x2 <= x1:
            logger.warning(f"ìœ íš¨í•˜ì§€ ì•Šì€ BBox í¬ê¸°: Element {element.element_id}")
            return ""

        # ì´ë¯¸ì§€ í¬ë¡­
        cropped_img = image[y1:y2, x1:x2]

        # 2. PIL ì´ë¯¸ì§€ ë³€í™˜ ë° Base64 ì¸ì½”ë”©
        pil_img = Image.fromarray(cv2.cvtColor(cropped_img, cv2.COLOR_BGR2RGB))
        buffered = io.BytesIO()
        pil_img.save(buffered, format="PNG")
        img_base64 = base64.b64encode(buffered.getvalue()).decode("utf-8")

        # 3. í”„ë¡¬í”„íŠ¸ ì„ íƒ
        prompts = {
            'figure': figure_prompt,
            'table': table_prompt,
            'flowchart': flowchart_prompt
        }
        prompt = prompts.get(element.class_name, f"ì´ {element.class_name} ë‚´ìš© ì„¤ëª…")

        system_prompt = (
            "ë‹¹ì‹ ì€ ì‹œê° ì¥ì•  ì•„ë™ í•™ìŠµ AI ë¹„ì„œì…ë‹ˆë‹¤. "
            "ì‹œê° ìë£Œ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ê°„ê²°, ëª…í™•í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”. "
            "ìŒì„± ë³€í™˜ ê°€ëŠ¥í•˜ê²Œ ì§ì ‘ì ì´ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì‘ì„±í•˜ì„¸ìš”."
        )

        # 4. ì§€ìˆ˜ ë°±ì˜¤í”„ ì¬ì‹œë„ ë¡œì§
        max_retries = 3
        base_delay = 1.0  # ì´ˆ ë‹¨ìœ„

        async with semaphore:  # Rate Limit ì œì–´
            for attempt in range(max_retries):
                try:
                    # API í˜¸ì¶œ
                    response = await async_client.chat.completions.create(
                        model="gpt-4-turbo",  # ë˜ëŠ” gpt-4o
                        messages=[
                            {"role": "system", "content": system_prompt},
                            {
                                "role": "user",
                                "content": [
                                    {"type": "text", "text": prompt},
                                    {
                                        "type": "image_url",
                                        "image_url": {
                                            "url": f"data:image/png;base64,{img_base64}"
                                        }
                                    }
                                ]
                            }
                        ],
                        temperature=0.2,
                        max_tokens=600
                    )

                    # ì„±ê³µ ì‹œ ê²°ê³¼ ë°˜í™˜
                    description = response.choices[0].message.content.strip()
                    logger.debug(
                        f"API ì‘ë‹µ ì™„ë£Œ (ì‹œë„ {attempt + 1}/{max_retries}): "
                        f"Element {element.element_id}"
                    )
                    return description

                except openai.RateLimitError as e:
                    # Rate Limit ì˜¤ë¥˜: ì§€ìˆ˜ ë°±ì˜¤í”„ ëŒ€ê¸° í›„ ì¬ì‹œë„
                    if attempt < max_retries - 1:
                        delay = base_delay * (2 ** attempt)  # 1ì´ˆ â†’ 2ì´ˆ â†’ 4ì´ˆ
                        logger.warning(
                            f"âš ï¸ Rate Limit ì˜¤ë¥˜ (Element {element.element_id}): "
                            f"{delay}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„ ({attempt + 1}/{max_retries})"
                        )
                        await asyncio.sleep(delay)
                    else:
                        logger.error(
                            f"âŒ Rate Limit ì˜¤ë¥˜ ìµœì¢… ì‹¤íŒ¨ (Element {element.element_id}): {e}"
                        )
                        raise  # ìµœì¢… ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸ ì „íŒŒ

                except openai.APIError as e:
                    # API ì¼ë°˜ ì˜¤ë¥˜: ì§€ìˆ˜ ë°±ì˜¤í”„ ëŒ€ê¸° í›„ ì¬ì‹œë„
                    if attempt < max_retries - 1:
                        delay = base_delay * (2 ** attempt)
                        logger.warning(
                            f"âš ï¸ API ì˜¤ë¥˜ (Element {element.element_id}): "
                            f"{delay}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„ ({attempt + 1}/{max_retries}) - {e}"
                        )
                        await asyncio.sleep(delay)
                    else:
                        logger.error(
                            f"âŒ API ì˜¤ë¥˜ ìµœì¢… ì‹¤íŒ¨ (Element {element.element_id}): {e}"
                        )
                        raise

                except Exception as e:
                    # ê¸°íƒ€ ì˜ˆì™¸: ì¦‰ì‹œ ì‹¤íŒ¨
                    logger.error(
                        f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ (Element {element.element_id}): {e}",
                        exc_info=True
                    )
                    raise

        # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜ (unreachable, but for type safety)
        return ""

    def visualize_results(self, image: np.ndarray, layout_elements: List[MockElement]) -> np.ndarray:
        """ê²°ê³¼ ì‹œê°í™” (ê¸°ì¡´ê³¼ ë™ì¼)"""
        img_result = image.copy(); overlay = image.copy()
        random.seed(42)
        unique_classes = list({elem.class_name for elem in layout_elements})
        class_colors = {}
        for i, cls_name in enumerate(unique_classes):
            h, s, v = i / max(1, len(unique_classes)), 0.8, 0.9
            r, g, b = colorsys.hsv_to_rgb(h, s, v)
            class_colors[cls_name] = (int(b * 255), int(g * 255), int(r * 255))
        for element in layout_elements:
            x1, y1 = element.bbox_x, element.bbox_y
            x2, y2 = x1 + element.bbox_width, y1 + element.bbox_height
            cls_name, color = element.class_name, class_colors[element.class_name]
            cv2.rectangle(overlay, (x1, y1), (x2, y2), color, -1)
            cv2.rectangle(img_result, (x1, y1), (x2, y2), color, 2)
            label = f"{cls_name} ({element.confidence:.2f})"
            labelSize, _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)
            y1_label = max(y1, labelSize[1] + 10)
            cv2.rectangle(img_result, (x1, y1_label - labelSize[1] - 10), (x1 + labelSize[0], y1_label), color, -1)
            cv2.putText(img_result, label, (x1, y1_label - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        img_result = cv2.addWeighted(overlay, 0.2, img_result, 0.8, 0)
        return cv2.cvtColor(img_result, cv2.COLOR_BGR2RGB)