# -*- coding: utf-8 -*-
"""
SmartEyeSsen Analysis Service (v1.1 - Duplicate Detection Filter Added)
========================================================================

í•™ìŠµì§€ ë¶„ì„ ì„œë¹„ìŠ¤ - ë ˆì´ì•„ì›ƒ ë¶„ì„, OCR, AI ì„¤ëª… ìƒì„±ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
Refactored from api_server.py WorksheetAnalyzer class.

ì£¼ìš” ë³€ê²½ì‚¬í•­ (DB í†µí•© ë²„ì „):
- analyze_layout: DocLayout-YOLO ê²°ê³¼ë¥¼ layout_elements í…Œì´ë¸”ì— ì €ì¥ í›„ ORM ê°ì²´ ë°˜í™˜
- perform_ocr: text_contents í…Œì´ë¸”ì— OCR ê²°ê³¼ upsert
- call_openai_api / call_openai_api_async: ai_descriptions í…Œì´ë¸”ì— ì„¤ëª… í…ìŠ¤íŠ¸ upsert
- ì¤‘ë³µ íƒì§€ í•„í„°ë§(IoU ê¸°ë°˜) ë¡œì§ ìœ ì§€
"""

import asyncio
import base64
import colorsys
import io
import platform
import random
from typing import Dict, List, Optional

import cv2
import numpy as np
import openai
import pytesseract
import torch
from PIL import Image
from huggingface_hub import hf_hub_download
from loguru import logger
from openai import AsyncOpenAI
from sqlalchemy.orm import Session

from .. import models

# --- ì‹ ê·œ: ì´ë¯¸ì§€ ì„¤ëª…ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì¶”ê°€ ---
figure_prompt = """
ë‹¹ì‹ ì€ ì‹œê° ì¥ì• ì¸ì„ ìœ„í•œ ì ìë„ì„œ ì‘ì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒ ì´ë¯¸ì§€ë¥¼ ì‹œê° ì¥ì• ì¸ì´ ì™„ì „íˆ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì„¤ëª…í•´ì£¼ì„¸ìš”.

[ì„¤ëª… ê·œì¹™]
1. ì „ì²´ êµ¬ì¡°: ì´ë¯¸ì§€ì˜ ì „ë°˜ì ì¸ í˜•íƒœì™€ êµ¬ì„±ì„ ë¨¼ì € ì„¤ëª…
2. ê³µê°„ ê´€ê³„: ìƒí•˜ì¢Œìš°, ì¤‘ì•™ ë“± ìœ„ì¹˜ ê´€ê³„ë¥¼ ëª…í™•íˆ í‘œí˜„
3. í•µì‹¬ ìš”ì†Œ: ê°€ì¥ ì¤‘ìš”í•œ ì‹œê° ì •ë³´ë¥¼ ìš°ì„ ìˆœìœ„ëŒ€ë¡œ ì„¤ëª…
4. ì„¸ë¶€ ì‚¬í•­: ìƒ‰ìƒ, í¬ê¸°, ëª¨ì–‘ì„ ì´‰ê°ì  í‘œí˜„ìœ¼ë¡œ ë³€í™˜
   - ì˜ˆ: "ì§„í•œ ë¹¨ê°„ìƒ‰" â†’ "ê°•ì¡°ëœ ë¶€ë¶„"
   - ì˜ˆ: "í° ì›" â†’ "ì†ë°”ë‹¥ í¬ê¸°ì˜ ë‘¥ê·¼ í˜•íƒœ"
5. ì˜ë¯¸ ì „ë‹¬: ì´ë¯¸ì§€ê°€ ì „ë‹¬í•˜ë ¤ëŠ” í•µì‹¬ ë©”ì‹œì§€ ìš”ì•½

[ì¶œë ¥ í˜•ì‹]
ì œëª©: [ì´ë¯¸ì§€ì˜ ì£¼ì œ]
êµ¬ì¡°: [ì „ì²´ì ì¸ ë ˆì´ì•„ì›ƒ]
ì£¼ìš” ìš”ì†Œ: [í•µì‹¬ êµ¬ì„±ìš”ì†Œ ë‚˜ì—´]
ìƒì„¸ ì„¤ëª…: [ê° ìš”ì†Œì˜ ê´€ê³„ì™€ ì˜ë¯¸]
í•µì‹¬ ë©”ì‹œì§€: [ì´ë¯¸ì§€ì˜ ëª©ì /ì˜ë„]

[ì˜ˆì‹œ]
ì œëª©: í•œêµ­ì˜ ì¸êµ¬ ì¦ê°€ ê·¸ë˜í”„
êµ¬ì¡°: ê°€ë¡œì¶•ì— ì—°ë„(2000-2025), ì„¸ë¡œì¶•ì— ì¸êµ¬ìˆ˜ê°€ í‘œì‹œëœ ì„  ê·¸ë˜í”„
ì£¼ìš” ìš”ì†Œ: 2000ë…„ 4,700ë§Œì—ì„œ ì‹œì‘í•˜ì—¬ 2025ë…„ 5,200ë§Œê¹Œì§€ ìƒìŠ¹í•˜ëŠ” ê³¡ì„ 
ìƒì„¸ ì„¤ëª…: 2010ë…„ê¹Œì§€ ê¸‰ê²©í•œ ìƒìŠ¹, ì´í›„ ì™„ë§Œí•œ ì¦ê°€ì„¸ë¥¼ ë³´ì„
í•µì‹¬ ë©”ì‹œì§€: í•œêµ­ ì¸êµ¬ëŠ” 25ë…„ê°„ ì•½ 10% ì¦ê°€í–ˆìœ¼ë©°, ìµœê·¼ ì¦ê°€ìœ¨ì´ ë‘”í™”ë¨
"""

table_prompt = """
ë‹¹ì‹ ì€ ì‹œê° ì¥ì• ì¸ì„ ìœ„í•œ ì ìë„ì„œ ì‘ì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒ í‘œë¥¼ ì‹œê° ì¥ì• ì¸ì´ ì ìë¡œ ì½ê¸° ì‰½ë„ë¡ ë³€í™˜í•´ì£¼ì„¸ìš”.

[ë³€í™˜ ê·œì¹™]
1. êµ¬ì¡° ìš°ì„ : í–‰ê³¼ ì—´ì˜ ê°œìˆ˜ë¥¼ ë¨¼ì € ëª…ì‹œ
2. í—¤ë” êµ¬ë¶„: í‘œ ë¨¸ë¦¬ê¸€ì„ ëª…í™•íˆ êµ¬ë¶„í•˜ì—¬ ì œì‹œ
3. ë°ì´í„° ì •ë¦¬: 
   - ìˆ«ìëŠ” ë‹¨ìœ„ì™€ í•¨ê»˜ ëª…ì‹œ
   - ë°±ë¶„ìœ¨ì€ "í¼ì„¼íŠ¸"ë¡œ í‘œê¸°
   - ë¹ˆ ì…€ì€ "ì—†ìŒ"ìœ¼ë¡œ í‘œê¸°
4. ìˆœì°¨ì  ì½ê¸°: ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½, ìœ„ì—ì„œ ì•„ë˜ë¡œ
5. ê´€ê³„ ì„¤ëª…: ë°ì´í„° ê°„ ë¹„êµë‚˜ ì¶”ì„¸ í¬í•¨

[ì¶œë ¥ í˜•ì‹]
í‘œ ì œëª©: [í‘œì˜ ì£¼ì œ]
í‘œ êµ¬ì¡°: [í–‰ ê°œìˆ˜] ê³±í•˜ê¸° [ì—´ ê°œìˆ˜]
ì—´ ì œëª©: [ê° ì—´ì˜ í—¤ë”]
ë°ì´í„°:
  í–‰1: [ë°ì´í„°1], [ë°ì´í„°2], ...
  í–‰2: [ë°ì´í„°1], [ë°ì´í„°2], ...
ì£¼ìš” ë°œê²¬: [í‘œì—ì„œ ì¤‘ìš”í•œ íŒ¨í„´ì´ë‚˜ ì •ë³´]

[ì˜ˆì‹œ]
í‘œ ì œëª©: 2024ë…„ ë¶„ê¸°ë³„ ë§¤ì¶œ ì‹¤ì 
í‘œ êµ¬ì¡°: 5í–‰ ê³±í•˜ê¸° 4ì—´
ì—´ ì œëª©: êµ¬ë¶„, 1ë¶„ê¸°, 2ë¶„ê¸°, 3ë¶„ê¸°
ë°ì´í„°:
  í–‰1(ë§¤ì¶œì•¡): 100ì–µì›, 120ì–µì›, 150ì–µì›
  í–‰2(ì„±ì¥ë¥ ): 10í¼ì„¼íŠ¸, 20í¼ì„¼íŠ¸, 25í¼ì„¼íŠ¸
  í–‰3(ì˜ì—…ì´ìµ): 10ì–µì›, 15ì–µì›, 20ì–µì›
  í–‰4(ìˆœì´ìµ): 8ì–µì›, 12ì–µì›, 18ì–µì›
ì£¼ìš” ë°œê²¬: ë§¤ì¶œì•¡ì´ ë§¤ ë¶„ê¸° ì§€ì†ì ìœ¼ë¡œ ì¦ê°€í•˜ë©°, 3ë¶„ê¸°ì— ê°€ì¥ ë†’ì€ ì„±ì¥ë¥  ê¸°ë¡
"""

flowchart_prompt = """
ë‹¹ì‹ ì€ ì‹œê° ì¥ì• ì¸ì„ ìœ„í•œ ì ìë„ì„œ ì‘ì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤.  
ë‹¤ìŒ ìˆœì„œë„(Flowchart) ë˜ëŠ” ê°œë…ë„(Concept Map)ë¥¼ ì‹œê° ì¥ì• ì¸ì´ ì‹œê°ì  êµ¬ì¡°ì™€ ë…¼ë¦¬ì  íë¦„ì„ ëª¨ë‘ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì„¤ëª…í•´ì£¼ì„¸ìš”.  

[ì„¤ëª… ê·œì¹™]
1. ì „ì²´ êµ¬ì¡° (Structure First): ê°€ì¥ ë¨¼ì €, ë‹¤ì´ì–´ê·¸ë¨ì˜ ì „ë°˜ì ì¸ ë ˆì´ì•„ì›ƒ(ì˜ˆ: 'ìƒë‹¨ì— 1ê°œ, í•˜ë‹¨ì— 2ê°œì˜ ìƒì')ì„ ì„¤ëª…í•©ë‹ˆë‹¤.  
2. ê³µê°„ ê´€ê³„ (Spatial Flow): í™”ì‚´í‘œ(ì—°ê²°ì„ )ì˜ ë°©í–¥ì„ "Aì—ì„œ Bë¡œ ì´ë™", "ë‘ ìš”ì†Œê°€ Cë¡œ í•©ì³ì§", "Aê°€ Bì™€ Cë¡œ ë‚˜ë‰¨" ë“±ìœ¼ë¡œ ëª…í™•íˆ ì„¤ëª…í•©ë‹ˆë‹¤.  
3. í•µì‹¬ ìš”ì†Œ (Key Elements): ê° ìƒì(ë…¸ë“œ) ì•ˆì˜ í•µì‹¬ ë‚´ìš©(í…ìŠ¤íŠ¸, ìˆ«ì, ë˜ëŠ” ì´ë¯¸ì§€ ë‚´ìš©)ì„ ì„¤ëª…í•©ë‹ˆë‹¤.  
4. ë¶„ê¸°ì  (Branching): ë§Œì•½ ê²°ì •(ë‹¤ì´ì•„ëª¬ë“œ í˜•íƒœ) ë…¸ë“œê°€ ìˆë‹¤ë©´, "ë§Œì•½...ë¼ë©´"ìœ¼ë¡œ ì¡°ê±´ì„ ì„¤ëª…í•˜ê³  ê° ì„ íƒì§€(ì˜ˆ/ì•„ë‹ˆì˜¤)ì˜ ê²½ë¡œë¥¼ ëª…í™•íˆ êµ¬ë¶„í•©ë‹ˆë‹¤.  
5. ì˜ë¯¸ ì „ë‹¬ (Core Message): ì´ ë‹¤ì´ì–´ê·¸ë¨ì´ ì „ë‹¬í•˜ë ¤ëŠ” í•µì‹¬ ì˜ë¯¸ë‚˜ ëª©ì ì„ ìš”ì•½í•©ë‹ˆë‹¤.

[ì¶œë ¥ í˜•ì‹]  
ì œëª©: [ë‹¤ì´ì–´ê·¸ë¨ì˜ ì£¼ì œ ë˜ëŠ” ëª©ì ]  
ì „ì²´ êµ¬ì¡°: [ì‹œê°ì  ë ˆì´ì•„ì›ƒ ì„¤ëª…. ì˜ˆ: 3ê°œì˜ ìƒìê°€ ìœ„ì—ì„œ ì•„ë˜ë¡œ ì—°ê²°ëœ êµ¬ì¡°]  
íë¦„ ì„¤ëª…:  
ì‹œì‘: [ì‹œì‘ ìƒì(ë“¤)ì˜ ë‚´ìš©]  
ì¤‘ê°„: [í™”ì‚´í‘œë¥¼ ë”°ë¼ê°€ë©° ë‹¤ìŒ ìƒìì˜ ë‚´ìš©ê³¼ ê´€ê³„ ì„¤ëª…. ë¶„ê¸°ì ì´ ìˆë‹¤ë©´ ì´ê³³ì—ì„œ ì„¤ëª…]  
ì¢…ë£Œ: [ìµœì¢… ìƒì(ë“¤)ì˜ ë‚´ìš©]  
í•µì‹¬ ìš”ì•½: [ë‹¤ì´ì–´ê·¸ë¨ì´ ì „ë‹¬í•˜ë ¤ëŠ” í•µì‹¬ ë©”ì‹œì§€ ë˜ëŠ” ê²°ë¡ ]  

[ì˜ˆì‹œ 1: ê°œë…ë„]  
ì œëª©: ë§ì…ˆ ê°œë…  
ì „ì²´ êµ¬ì¡°: ìƒë‹¨ì— ë‘ ê°œì˜ ë¶„ë¦¬ëœ ìƒìê°€ ìˆê³ , ë‘ ê°œì˜ í™”ì‚´í‘œê°€ ì´ ìƒìë“¤ì„ í•˜ë‹¨ì˜ í° ìƒì í•˜ë‚˜ë¡œ ëª¨ìœ¼ëŠ” êµ¬ì¡°ì…ë‹ˆë‹¤.  
íë¦„ ì„¤ëª…:  
ì‹œì‘: ìƒë‹¨ ì™¼ìª½ ìƒìì—ëŠ” 'í•­ëª© A (ì˜ˆ: ë¬´ë‹¹ë²Œë ˆ 1ë§ˆë¦¬)'ê°€ ìˆìŠµë‹ˆë‹¤. ìƒë‹¨ ì˜¤ë¥¸ìª½ ìƒìì—ëŠ” 'í•­ëª© B (ì˜ˆ: ë¬´ë‹¹ë²Œë ˆ 3ë§ˆë¦¬)'ê°€ ìˆìŠµë‹ˆë‹¤.  
ì¤‘ê°„: ì´ ë‘ ìƒìì—ì„œ ë‚˜ì˜¨ í™”ì‚´í‘œê°€ í•˜ë‹¨ ìƒìë¡œ í•©ì³ì§‘ë‹ˆë‹¤.  
ì¢…ë£Œ: í•˜ë‹¨ ìƒìì—ëŠ” 'ê²°ê³¼ C (ì˜ˆ: ë¬´ë‹¹ë²Œë ˆ 4ë§ˆë¦¬)'ê°€ ìˆìŠµë‹ˆë‹¤.  
í•µì‹¬ ìš”ì•½: í•­ëª© Aì™€ í•­ëª© Bë¥¼ ë”í•˜ë©´ ê²°ê³¼ Cê°€ ë¨ì„ ë³´ì—¬ì£¼ëŠ” ë§ì…ˆ ë‹¤ì´ì–´ê·¸ë¨ì…ë‹ˆë‹¤.

[ì˜ˆì‹œ 2: ë¶„ê¸°í˜•]  
ì œëª©: ìˆ«ì ë‚˜ëˆ„ê¸°  
ì „ì²´ êµ¬ì¡°: ìƒë‹¨ì— 1ê°œì˜ ìƒìê°€ ìˆê³ , ì´ ìƒìì—ì„œ 2ê°œì˜ í™”ì‚´í‘œê°€ ë‚˜ì™€ í•˜ë‹¨ì˜ ë¶„ë¦¬ëœ 2ê°œ ìƒìë¡œ ê°ê° ì—°ê²°ë˜ëŠ” Yì í˜•íƒœì…ë‹ˆë‹¤.  
íë¦„ ì„¤ëª…:  
ì‹œì‘: ìƒë‹¨ ìƒìì—ëŠ” ìˆ«ì '5'ê°€ ìˆìŠµë‹ˆë‹¤.  
ì¤‘ê°„: ìƒì '5'ì—ì„œ ë‘ ê°œì˜ í™”ì‚´í‘œê°€ ë‚˜ì™€ ê°ê° í•˜ë‹¨ì˜ ë¹ˆ ìƒìë¡œ ë‚˜ë‰©ë‹ˆë‹¤.  
ì¢…ë£Œ: í•˜ë‹¨ì˜ ì™¼ìª½ ë¹ˆ ìƒìì™€ ì˜¤ë¥¸ìª½ ë¹ˆ ìƒìë¡œ ì—°ê²°ë©ë‹ˆë‹¤. (ë‚´ìš©ì´ ë¹„ì–´ìˆìŒì„ ëª…ì‹œ)  
í•µì‹¬ ìš”ì•½: ìˆ«ì 5ë¥¼ ë‘ ë¶€ë¶„ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê³¼ì •ì„ ë³´ì—¬ì£¼ëŠ” ë‹¤ì´ì–´ê·¸ë¨ì…ë‹ˆë‹¤.
"""


# --- ì‹ ê·œ: IoU ê³„ì‚° í•¨ìˆ˜ ì¶”ê°€ ---
def calculate_iou(box1, box2):
    """ë‘ ë°”ìš´ë”© ë°•ìŠ¤ ê°„ì˜ IoU(Intersection over Union) ê³„ì‚°"""
    # box í˜•ì‹: [x1, y1, x2, y2]
    x1_inter = max(box1[0], box2[0])
    y1_inter = max(box1[1], box2[1])
    x2_inter = min(box1[2], box2[2])
    y2_inter = min(box1[3], box2[3])

    inter_area = max(0, x2_inter - x1_inter) * max(0, y2_inter - y1_inter)

    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])
    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])

    union_area = box1_area + box2_area - inter_area

    if union_area == 0:
        return 0.0
    return inter_area / union_area


# --- ì‹ ê·œ: ì¤‘ë³µ ì œê±° í›„ì²˜ë¦¬ í•¨ìˆ˜ ì¶”ê°€ ---
def filter_duplicate_detections(boxes, classes, confs, class_names, iou_threshold=0.7):
    """
    ëª¨ë“  í´ë˜ìŠ¤ ìŒì— ëŒ€í•´ IoU ê¸°ë°˜ìœ¼ë¡œ ì¤‘ë³µ íƒì§€ë¥¼ í•„í„°ë§. (ìë™ ë°©ì‹)
    ì‹ ë¢°ë„ê°€ ë‚®ì€ ìª½ì„ ì œê±°.
    """
    num_detections = len(boxes)
    suppressed = [False] * num_detections  # ì œê±°í•  ìš”ì†Œ í‘œì‹œ

    indices = list(range(num_detections))
    # ì‹ ë¢°ë„ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬ (ë†’ì€ ê²ƒì„ ë‚¨ê¸°ê¸° ìœ„í•¨)
    indices.sort(key=lambda i: confs[i], reverse=True)

    for i in range(num_detections):
        idx1 = indices[i]
        if suppressed[idx1]:
            continue

        box1 = boxes[idx1]
        # cls_id1 = int(classes[idx1]) # í´ë˜ìŠ¤ ì •ë³´ëŠ” ì œê±° ë¡œì§ì— ë¶ˆí•„ìš”
        # cls_name1 = class_names.get(cls_id1, f"unknown_{cls_id1}") # í´ë˜ìŠ¤ ì •ë³´ëŠ” ì œê±° ë¡œì§ì— ë¶ˆí•„ìš”

        for j in range(i + 1, num_detections):
            idx2 = indices[j]
            if suppressed[idx2]:
                continue

            box2 = boxes[idx2]
            # cls_id2 = int(classes[idx2]) # í´ë˜ìŠ¤ ì •ë³´ëŠ” ì œê±° ë¡œì§ì— ë¶ˆí•„ìš”
            # cls_name2 = class_names.get(cls_id2, f"unknown_{cls_id2}") # í´ë˜ìŠ¤ ì •ë³´ëŠ” ì œê±° ë¡œì§ì— ë¶ˆí•„ìš”

            # --- ğŸ‘‡ ìˆ˜ì •ëœ ë¶€ë¶„ ì‹œì‘ ğŸ‘‡ ---
            # íŠ¹ì • í´ë˜ìŠ¤ ìŒ í™•ì¸ ì¡°ê±´ ì œê±°: ëª¨ë“  ìŒì— ëŒ€í•´ IoU ê³„ì‚°
            # if (cls_name1, cls_name2) in problematic_pairs: # ì´ ì¡°ê±´ ì œê±°
            iou = calculate_iou(box1, box2)
            if iou > iou_threshold:
                # ì‹ ë¢°ë„ ë‚®ì€ ìª½(idx2)ì„ ì œê±° ëŒ€ìƒìœ¼ë¡œ í‘œì‹œ
                suppressed[idx2] = True
                # ë¡œê·¸ ë©”ì‹œì§€ì—ì„œ í´ë˜ìŠ¤ ì´ë¦„ ì œê±° (ì„ íƒ ì‚¬í•­)
                logger.debug(
                    f"ì¤‘ë³µ íƒì§€ ì œê±°: Box {idx2}(conf={confs[idx2]:.2f}) - "
                    f"Box {idx1}(conf={confs[idx1]:.2f})ì™€ IoU={iou:.2f} > {iou_threshold}"
                )
            # --- ğŸ‘† ìˆ˜ì •ëœ ë¶€ë¶„ ë ğŸ‘† ---

    # ì œê±°ë˜ì§€ ì•Šì€ ìš”ì†Œë“¤ì˜ ì¸ë±ìŠ¤ ë°˜í™˜
    final_indices = [i for i, s in enumerate(suppressed) if not s]
    logger.info(
        f"ìë™ ì¤‘ë³µ íƒì§€ í•„í„°ë§: {num_detections}ê°œ â†’ {len(final_indices)}ê°œ ìš”ì†Œ (IoU > {iou_threshold})"
    )  # ë¡œê·¸ ë©”ì‹œì§€ ìˆ˜ì •
    return final_indices


# Windowsì—ì„œ Tesseract ê²½ë¡œ ì„¤ì • (ê¸°ì¡´ê³¼ ë™ì¼)
if platform.system() == "Windows":
    pytesseract.pytesseract.tesseract_cmd = (
        r"C:\Program Files\Tesseract-OCR\tesseract.exe"
    )

# ë””ë°”ì´ìŠ¤ ì„¤ì • (ê¸°ì¡´ê³¼ ë™ì¼)
device = "cuda:0" if torch.cuda.is_available() else "cpu"


class AnalysisService:
    """í•™ìŠµì§€ ë¶„ì„ ì„œë¹„ìŠ¤ - ìƒíƒœ ì—†ëŠ” í•¨ìˆ˜í˜• ë””ìì¸"""

    def __init__(self, model_choice: str = "SmartEyeSsen", auto_load: bool = False):
        """
        ë¶„ì„ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”

        Args:
            model_choice: ì‚¬ìš©í•  ëª¨ë¸ ì„ íƒ (ê¸°ë³¸ê°’: "SmartEyeSsen")
            auto_load: Trueì´ë©´ ì´ˆê¸°í™” ì‹œ ìë™ìœ¼ë¡œ ëª¨ë¸ ë¡œë“œ (ê¸°ë³¸ê°’: False, í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)
        """
        self.model = None
        self.device = device
        self.model_choice = model_choice
        self._model_loaded = False

        # ìë™ ë¡œë“œ ì˜µì…˜ì´ í™œì„±í™”ëœ ê²½ìš° ì¦‰ì‹œ ëª¨ë¸ ë¡œë“œ
        if auto_load:
            self._ensure_model_loaded()

    def download_model(self, model_choice="SmartEyeSsen"):
        """ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ê¸°ì¡´ê³¼ ë™ì¼)"""
        models = {
            "doclaynet_docsynth": {
                "repo_id": "juliozhao/DocLayout-YOLO-DocLayNet-Docsynth300K_pretrained",
                "filename": "doclayout_yolo_doclaynet_imgsz1120_docsynth_pretrain.pt",
            },
            "docstructbench": {
                "repo_id": "juliozhao/DocLayout-YOLO-DocStructBench",
                "filename": "doclayout_yolo_docstructbench_imgsz1024.pt",
            },
            "docsynth300k": {
                "repo_id": "juliozhao/DocLayout-YOLO-DocSynth300K-pretrain",
                "filename": "doclayout_yolo_docsynth300k_imgsz1600.pt",
            },
            "SmartEyeSsen": {"repo_id": "AkJeond/SmartEye", "filename": "best.pt"},
        }
        selected_model = models.get(model_choice, models["SmartEyeSsen"])
        try:
            logger.info(f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘: {selected_model['repo_id']}")
            filepath = hf_hub_download(
                repo_id=selected_model["repo_id"], filename=selected_model["filename"]
            )
            logger.info(f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {filepath}")
            return filepath
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise

    def load_model(self, model_path):
        """ëª¨ë¸ ë¡œë“œ (ê¸°ì¡´ê³¼ ë™ì¼)"""
        try:
            try:
                from doclayout_yolo import YOLOv10
            except ImportError:
                logger.error("DocLayout-YOLOê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return False
            logger.info("ëª¨ë¸ ë¡œë“œ ì¤‘...")
            self.model = YOLOv10(model_path, task="predict")
            self.model.to(self.device)
            if hasattr(self.model, "training"):
                self.model.training = False
            logger.info("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
            return True
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    def _ensure_model_loaded(self):
        """
        Lazy Loading: ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìœ¼ë©´ ìë™ìœ¼ë¡œ ë¡œë“œ
        (ë‹¤ì¤‘ í˜ì´ì§€ ì²˜ë¦¬ ì‹œ ëª¨ë¸ì„ í•œ ë²ˆë§Œ ë¡œë“œí•˜ë„ë¡ ìµœì í™”)
        """
        if self._model_loaded and self.model is not None:
            return  # ì´ë¯¸ ë¡œë“œë¨

        logger.info(f"ëª¨ë¸ ìë™ ë¡œë“œ ì‹œì‘ (ì„ íƒ: {self.model_choice})...")
        model_path = self.download_model(self.model_choice)
        if not self.load_model(model_path):
            raise RuntimeError(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {self.model_choice}")
        self._model_loaded = True
        logger.info("ëª¨ë¸ ìë™ ë¡œë“œ ì™„ë£Œ!")

    def analyze_layout(
        self,
        image: np.ndarray,
        *,
        page_id: int,
        db: Session,
        model_choice: Optional[str] = None,
    ) -> List[models.LayoutElement]:
        """
        ë ˆì´ì•„ì›ƒ ë¶„ì„ + ì¤‘ë³µ íƒì§€ í•„í„°ë§ í›„ ê²°ê³¼ë¥¼ DBì— ì €ì¥í•œë‹¤.

        Args:
            image: ë¶„ì„í•  ì´ë¯¸ì§€ (numpy array)
            page_id: ê²°ê³¼ë¥¼ ì €ì¥í•  pages.page_id
            db: SQLAlchemy Session
            model_choice: ì‚¬ìš©í•  ëª¨ë¸ (ë¯¸ì§€ì • ì‹œ ì¸ìŠ¤í„´ìŠ¤ ê¸°ë³¸ê°’ ì‚¬ìš©)

        Returns:
            DBì— ì €ì¥ëœ LayoutElement ORM ê°ì²´ ë¦¬ìŠ¤íŠ¸
        """
        active_model = model_choice or self.model_choice

        try:
            # ëª¨ë¸ ì„ íƒì´ ë³€ê²½ë˜ì—ˆìœ¼ë©´ ì¬ë¡œë“œ
            if active_model != self.model_choice:
                logger.warning(f"ëª¨ë¸ ë³€ê²½ ê°ì§€: {self.model_choice} -> {active_model}")
                self.model_choice = active_model
                self._model_loaded = False

            # Lazy Loading: ëª¨ë¸ì´ ì—†ìœ¼ë©´ ìë™ ë¡œë“œ
            self._ensure_model_loaded()

            logger.info("ë ˆì´ì•„ì›ƒ ë¶„ì„ ì‹œì‘...")
            temp_path = "temp_image.jpg"
            cv2.imwrite(temp_path, image)

            if active_model == "SmartEyeSsen":
                imgsz, conf = 1024, 0.25
            elif active_model == "docsynth300k":
                imgsz, conf = 1600, 0.15
            else:
                imgsz, conf = 1024, 0.25

            results = self.model.predict(
                temp_path, imgsz=imgsz, conf=conf, iou=0.45, device=self.device
            )

            boxes = results[0].boxes.xyxy.cpu().numpy()  # [x1, y1, x2, y2]
            classes = results[0].boxes.cls.cpu().numpy()
            confs = results[0].boxes.conf.cpu().numpy()
            class_names = self.model.names  # í´ë˜ìŠ¤ ID â†’ ì´ë¦„

            detection_records: List[Dict[str, float]] = []

            if not boxes.size:
                logger.warning("ë ˆì´ì•„ì›ƒ ë¶„ì„ ê²°ê³¼, ê°ì§€ëœ ìš”ì†Œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return self._create_elements_from_layout(
                    detections=detection_records, page_id=page_id, db=db
                )

            final_indices = filter_duplicate_detections(
                boxes, classes, confs, class_names, iou_threshold=0.7
            )

            for i in final_indices:
                box = boxes[i]
                cls_id = int(classes[i])
                conf_val = float(confs[i])
                x1, y1, x2, y2 = map(int, box)

                cls_name = (
                    class_names.get(cls_id, f"unknown_{cls_id}")
                    if isinstance(class_names, dict)
                    else None
                )
                if cls_name is None:
                    try:
                        cls_name = class_names[cls_id]
                    except (IndexError, KeyError):
                        cls_name = f"unknown_{cls_id}"

                width = x2 - x1
                height = y2 - y1
                area = width * height
                if area < 100:
                    continue

                detection_records.append(
                    {
                        "class_name": cls_name,
                        "confidence": conf_val,
                        "bbox_x": x1,
                        "bbox_y": y1,
                        "bbox_width": width,
                        "bbox_height": height,
                    }
                )

            elements = self._create_elements_from_layout(
                detections=detection_records, page_id=page_id, db=db
            )
            logger.info(f"ë ˆì´ì•„ì›ƒ ë¶„ì„ ì™„ë£Œ: ìµœì¢… {len(elements)}ê°œ ìš”ì†Œ ì €ì¥")
            return elements

        except Exception as e:
            logger.error(f"ë ˆì´ì•„ì›ƒ ë¶„ì„ ì‹¤íŒ¨: {e}", exc_info=True)
            return []

    def _create_elements_from_layout(
        self, *, detections: List[Dict[str, float]], page_id: int, db: Session
    ) -> List[models.LayoutElement]:
        """
        ê°ì§€ ê²°ê³¼ë¥¼ layout_elements í…Œì´ë¸”ì— ì €ì¥í•˜ê³  ORM ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•œë‹¤.
        """
        logger.debug(f"í˜ì´ì§€ {page_id} ê¸°ì¡´ ë ˆì´ì•„ì›ƒ ìš”ì†Œ ì •ë¦¬")
        existing_elements = (
            db.query(models.LayoutElement)
            .filter(models.LayoutElement.page_id == page_id)
            .all()
        )
        for element in existing_elements:
            db.delete(element)
        db.flush()  # CASCADE ê´€ê³„ ì •ë¦¬

        if not detections:
            db.commit()
            return []

        created_elements: List[models.LayoutElement] = []
        for record in detections:
            element = models.LayoutElement(
                page_id=page_id,
                class_name=record["class_name"],
                confidence=record["confidence"],
                bbox_x=int(record["bbox_x"]),
                bbox_y=int(record["bbox_y"]),
                bbox_width=int(record["bbox_width"]),
                bbox_height=int(record["bbox_height"]),
            )
            db.add(element)
            created_elements.append(element)

        db.flush()
        db.commit()
        for element in created_elements:
            db.refresh(element)

        return created_elements

    def _upsert_text_content(
        self,
        *,
        db: Session,
        element_id: int,
        ocr_text: str,
        ocr_engine: str,
        language: str,
        ocr_confidence: Optional[float] = None,
    ) -> models.TextContent:
        """
        í…ìŠ¤íŠ¸ ì½˜í…ì¸ ë¥¼ ìƒì„±í•˜ê±°ë‚˜ ì—…ë°ì´íŠ¸í•œë‹¤.
        """
        existing = (
            db.query(models.TextContent)
            .filter(models.TextContent.element_id == element_id)
            .one_or_none()
        )

        if existing:
            existing.ocr_text = ocr_text
            existing.ocr_engine = ocr_engine
            existing.language = language
            existing.ocr_confidence = ocr_confidence
            db.flush()
            return existing

        content = models.TextContent(
            element_id=element_id,
            ocr_text=ocr_text,
            ocr_engine=ocr_engine,
            ocr_confidence=ocr_confidence,
            language=language,
        )
        db.add(content)
        db.flush()
        return content

    def _upsert_ai_descriptions(
        self,
        *,
        db: Session,
        descriptions: Dict[int, str],
        model_name: str,
        prompt: Optional[str],
    ) -> List[models.AIDescription]:
        """
        AI ì„¤ëª…ì„ ìƒì„±í•˜ê±°ë‚˜ ê°±ì‹ í•œë‹¤.
        """
        saved_records: List[models.AIDescription] = []
        for element_id, description in descriptions.items():
            existing = (
                db.query(models.AIDescription)
                .filter(models.AIDescription.element_id == element_id)
                .one_or_none()
            )

            if existing:
                existing.description = description
                existing.ai_model = model_name
                existing.prompt_used = prompt
                db.flush()
                saved_records.append(existing)
                continue

            record = models.AIDescription(
                element_id=element_id,
                description=description,
                ai_model=model_name,
                prompt_used=prompt,
            )
            db.add(record)
            saved_records.append(record)

        db.flush()
        db.commit()
        for record in saved_records:
            db.refresh(record)

        return saved_records

    def perform_ocr(
        self,
        image: np.ndarray,
        layout_elements: List[models.LayoutElement],
        *,
        db: Session,
        language: str = "kor",
    ) -> List[models.TextContent]:
        """OCR ì²˜ë¦¬ (ì˜ì—­ë³„ ì „ì²˜ë¦¬ ì¶”ê°€) ë° text_contents í…Œì´ë¸” ì €ì¥"""
        target_classes = [
            "plain text",
            "unit",
            "question type",
            "question text",
            "question number",
            "title",
            "figure_caption",
            "table caption",
            "table footnote",
            "isolate_formula",
            "formula_caption",
            "list",
            "choices",
            "page",
            "second_question_number",
        ]
        ocr_results: List[models.TextContent] = []
        custom_config = r"--oem 3 --psm 6"
        logger.info(
            f"OCR ì²˜ë¦¬ ì‹œì‘... ì´ {len(layout_elements)}ê°œ ë ˆì´ì•„ì›ƒ ìš”ì†Œ ì¤‘ OCR ëŒ€ìƒ í•„í„°ë§"
        )
        logger.info(f"OCR ëŒ€ìƒ í´ë˜ìŠ¤ ëª©ë¡: {target_classes}")
        detected_classes = {elem.class_name for elem in layout_elements}  # Setìœ¼ë¡œ ë³€ê²½
        logger.info(f"ê°ì§€ëœ ëª¨ë“  í´ë˜ìŠ¤: {detected_classes}")

        target_count = 0
        for element in layout_elements:
            cls_name = element.class_name  # Pydantic ëª¨ë¸ì€ ì´ë¯¸ lower() ë¶ˆí•„ìš”
            logger.debug(
                f"ë ˆì´ì•„ì›ƒ ID {element.element_id}: í´ë˜ìŠ¤ '{cls_name}' í™•ì¸ ì¤‘..."
            )  # DEBUG ë ˆë²¨ë¡œ ë³€ê²½
            if cls_name not in target_classes:
                logger.debug(f"  â†’ OCR ëŒ€ìƒ ì•„ë‹˜")
                continue

            target_count += 1
            logger.debug(
                f"  â†’ OCR ëŒ€ìƒ {target_count}: ID {element.element_id} - í´ë˜ìŠ¤ '{cls_name}'"
            )

            # 1. ì˜ì—­ ì´ë¯¸ì§€ ì˜ë¼ë‚´ê¸° (ê¸°ì¡´ ì½”ë“œ)
            x1, y1 = element.bbox_x, element.bbox_y
            x2, y2 = x1 + element.bbox_width, y1 + element.bbox_height
            # ì´ë¯¸ì§€ ê²½ê³„ ë‚´ë¡œ ì¢Œí‘œ ì¡°ì •
            x1, y1 = max(0, x1), max(0, y1)
            x2, y2 = min(image.shape[1], x2), min(image.shape[0], y2)

            if y2 <= y1 or x2 <= x1:  # í¬ê¸°ê°€ 0ì´ê±°ë‚˜ ìŒìˆ˜ì¸ ê²½ìš° ê±´ë„ˆë›°ê¸°
                logger.warning(
                    f"  â†’ ìœ íš¨í•˜ì§€ ì•Šì€ BBox í¬ê¸°: ID {element.element_id}, ê±´ë„ˆëœ€"
                )
                continue
            cropped_img = image[y1:y2, x1:x2]

            try:
                # --- ğŸ‘‡ ì˜ì—­ë³„ ì „ì²˜ë¦¬ ë‹¨ê³„ ì‹œì‘ ğŸ‘‡ ---

                # 2. ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜: ìƒ‰ìƒ ì •ë³´ ì œê±°
                gray_img = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2GRAY)

                # 3. ì´ì§„í™” (Otsu's Binarization): í…ìŠ¤íŠ¸/ë°°ê²½ ëª…í™•í™”
                # Otsu ë°©ì‹ì€ ì„ê³„ê°’ì„ ìë™ìœ¼ë¡œ ê²°ì •í•´ ì¤ë‹ˆë‹¤.
                # í•„ìš”ì— ë”°ë¼ cv2.adaptiveThreshold ë“± ë‹¤ë¥¸ ë°©ì‹ ì‚¬ìš© ê°€ëŠ¥
                _, binary_img = cv2.threshold(
                    gray_img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU
                )

                # 4. (ì„ íƒì ) ë…¸ì´ì¦ˆ ì œê±°: Median í•„í„° ì ìš© (ì‘ì€ ì  ì œê±°ì— íš¨ê³¼ì )
                # ì»¤ë„ í¬ê¸°(ì˜ˆ: 3)ëŠ” ì‹¤í—˜ì„ í†µí•´ ì¡°ì •
                denoised_img = cv2.medianBlur(binary_img, 3)

                # --- ğŸ‘† ì˜ì—­ë³„ ì „ì²˜ë¦¬ ë‹¨ê³„ ë ğŸ‘† ---

                # 5. ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ë¡œ OCR ìˆ˜í–‰
                # Pillow ì´ë¯¸ì§€ë¡œ ë³€í™˜ (TesseractëŠ” Pillow ì´ë¯¸ì§€ ì…ë ¥ ì„ í˜¸)
                pil_img = Image.fromarray(cropped_img)
                text = pytesseract.image_to_string(
                    pil_img, lang="kor", config=custom_config
                ).strip()

                if len(text) > 1:  # ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš°ë§Œ
                    db_text = self._upsert_text_content(
                        db=db,
                        element_id=element.element_id,
                        ocr_text=text,
                        ocr_engine="Tesseract",
                        language=language,
                    )
                    ocr_results.append(db_text)
                    logger.info(
                        f"âœ… OCR ì„±ê³µ: ID {element.element_id} ({cls_name}) - '{text[:50].replace(chr(10), ' ')}...' ({len(text)}ì)"
                    )  # ê°œí–‰ë¬¸ì ì œê±°
                else:
                    logger.warning(
                        f"âš ï¸ OCR ê²°ê³¼ ì—†ìŒ: ID {element.element_id} ({cls_name})"
                    )
            except Exception as e:
                logger.error(
                    f"OCR ì‹¤íŒ¨: ID {element.element_id} - {e}", exc_info=True
                )  # ìƒì„¸ ì—ëŸ¬

        db.commit()
        for content in ocr_results:
            db.refresh(content)

        logger.info(f"OCR ì²˜ë¦¬ ì™„ë£Œ: {len(ocr_results)}ê°œ í…ìŠ¤íŠ¸ ë¸”ë¡ ì €ì¥")
        return ocr_results

    def call_openai_api(
        self,
        image: np.ndarray,
        layout_elements: List[models.LayoutElement],
        *,
        api_key: Optional[str],
        db: Session,
        model_name: str = "gpt-4-turbo",
    ) -> Dict[int, str]:
        """OpenAI API í˜¸ì¶œ ë° ai_descriptions í…Œì´ë¸” ì €ì¥"""
        if not api_key:
            logger.warning("API í‚¤ê°€ ì—†ì–´ AI ì„¤ëª… ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return {}
        target_classes = ["figure", "table", "flowchart"]
        ai_descriptions: Dict[int, str] = {}

        try:
            client = openai.OpenAI(api_key=api_key)
            logger.info("OpenAI API ì²˜ë¦¬ ì‹œì‘...")
        except Exception as e:
            logger.error(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return {}

        prompts = {
            "figure": figure_prompt,
            "table": table_prompt,
            "flowchart": flowchart_prompt,
        }
        system_prompt = (
            "ë‹¹ì‹ ì€ ì‹œê° ì¥ì•  ì•„ë™ í•™ìŠµ AI ë¹„ì„œì…ë‹ˆë‹¤. "
            "ì‹œê° ìë£Œ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”. "
            "ìŒì„± ë³€í™˜ ì‹œ ì´í•´í•˜ê¸° ì‰½ë„ë¡ ì§ì ‘ì ì¸ ë¬¸ì¥ì„ ì‚¬ìš©í•˜ì„¸ìš”."
        )

        for element in layout_elements:
            cls_name = element.class_name
            if cls_name not in target_classes:
                continue

            x1, y1 = element.bbox_x, element.bbox_y
            x2, y2 = x1 + element.bbox_width, y1 + element.bbox_height
            if y2 <= y1 or x2 <= x1:
                continue

            cropped_img = image[y1:y2, x1:x2]
            pil_img = Image.fromarray(cv2.cvtColor(cropped_img, cv2.COLOR_BGR2RGB))
            buffered = io.BytesIO()
            pil_img.save(buffered, format="PNG")
            img_base64 = base64.b64encode(buffered.getvalue()).decode("utf-8")
            prompt = prompts.get(cls_name, f"ì´ {cls_name} ë‚´ìš© ì„¤ëª…")

            try:
                response = client.chat.completions.create(
                    model=model_name,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {
                            "role": "user",
                            "content": [
                                {"type": "text", "text": prompt},
                                {
                                    "type": "image_url",
                                    "image_url": {
                                        "url": f"data:image/png;base64,{img_base64}"
                                    },
                                },
                            ],
                        },
                    ],
                    temperature=0.2,
                    max_tokens=600,
                )
                description = response.choices[0].message.content.strip()
                ai_descriptions[element.element_id] = description
                logger.info(f"API ì‘ë‹µ ì™„ë£Œ: ID {element.element_id} - {cls_name}")
            except Exception as e:
                logger.error(
                    f"API ìš”ì²­ ì‹¤íŒ¨: ID {element.element_id} - {e}", exc_info=True
                )

        saved = self._upsert_ai_descriptions(
            db=db, descriptions=ai_descriptions, model_name=model_name, prompt=None
        )
        logger.info(f"OpenAI API ì²˜ë¦¬ ì™„ë£Œ: {len(saved)}ê°œ ì„¤ëª… ìƒì„± ë° ì €ì¥")
        return ai_descriptions

    async def call_openai_api_async(
        self,
        image: np.ndarray,
        layout_elements: List[models.LayoutElement],
        api_key: str,
        *,
        db: Optional[Session] = None,
        model_name: str = "gpt-4-turbo",
        max_concurrent_requests: int = 5,
    ) -> Dict[int, str]:
        """
        OpenAI API ë¹„ë™ê¸° ë³‘ë ¬ í˜¸ì¶œ (ì„±ëŠ¥ ìµœì í™” ë²„ì „)

        Args:
            image: ì›ë³¸ ì´ë¯¸ì§€ (BGR í¬ë§·)
            layout_elements: ë ˆì´ì•„ì›ƒ ìš”ì†Œ ë¦¬ìŠ¤íŠ¸
            api_key: OpenAI API í‚¤
            db: SQLAlchemy Session (ì„ íƒ, ì œê³µ ì‹œ DBì— ì„¤ëª… ì €ì¥)
            model_name: ì‚¬ìš©í•  OpenAI ëª¨ë¸ ì´ë¦„
            max_concurrent_requests: ìµœëŒ€ ë™ì‹œ ìš”ì²­ ìˆ˜ (ê¸°ë³¸ê°’: 5)

        Returns:
            Dict[int, str]: {element_id: AI ì„¤ëª…} ë”•ì…”ë„ˆë¦¬

        ì£¼ìš” ê°œì„ ì‚¬í•­:
        - ë¹„ë™ê¸° ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì²˜ë¦¬ ì‹œê°„ 70% ë‹¨ì¶•
        - asyncio.Semaphoreë¡œ Rate Limit ëŒ€ì‘
        - ì§€ìˆ˜ ë°±ì˜¤í”„ ì¬ì‹œë„ ë¡œì§ (exponential backoff)
        """
        if not api_key:
            logger.warning("API í‚¤ê°€ ì—†ì–´ AI ì„¤ëª… ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return {}

        # 1. ëŒ€ìƒ í´ë˜ìŠ¤ í•„í„°ë§ (figure, table, flowchartë§Œ ì²˜ë¦¬)
        target_classes = ["figure", "table", "flowchart"]
        target_elements = [
            elem for elem in layout_elements if elem.class_name in target_classes
        ]

        if not target_elements:
            logger.info("AI ì„¤ëª… ëŒ€ìƒ ìš”ì†Œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {}

        logger.info(
            f"OpenAI API ë¹„ë™ê¸° ì²˜ë¦¬ ì‹œì‘... (ì´ {len(target_elements)}ê°œ ìš”ì†Œ)"
        )

        # 2. AsyncOpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        try:
            async_client = AsyncOpenAI(api_key=api_key)
        except Exception as e:
            logger.error(f"AsyncOpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return {}

        # 3. Semaphoreë¡œ ë™ì‹œ ìš”ì²­ ìˆ˜ ì œí•œ (Rate Limit ëŒ€ì‘)
        semaphore = asyncio.Semaphore(max_concurrent_requests)

        # 4. ëª¨ë“  ë¹„ë™ê¸° íƒœìŠ¤í¬ ìƒì„±
        tasks = [
            self._process_single_element_async(
                async_client=async_client,
                image=image,
                element=elem,
                semaphore=semaphore,
                model_name=model_name,
            )
            for elem in target_elements
        ]

        # 5. ë³‘ë ¬ ì‹¤í–‰ (asyncio.gather)
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # 6. ê²°ê³¼ ë§¤í•‘ ë° ì˜ˆì™¸ ì²˜ë¦¬
        ai_descriptions = {}
        success_count = 0
        error_count = 0

        for element, result in zip(target_elements, results):
            if isinstance(result, Exception):
                logger.error(f"API ì‹¤íŒ¨: Element {element.element_id} - {result}")
                error_count += 1
            elif result:  # ì„±ê³µ ì‹œ (ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš°)
                ai_descriptions[element.element_id] = result
                success_count += 1
                logger.info(
                    f"âœ… API ì„±ê³µ: Element {element.element_id} ({element.class_name})"
                )

        logger.info(
            f"OpenAI API ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ë£Œ: "
            f"ì„±ê³µ {success_count}ê±´, ì‹¤íŒ¨ {error_count}ê±´ / ì´ {len(target_elements)}ê±´"
        )

        if db and ai_descriptions:
            saved = self._upsert_ai_descriptions(
                db=db, descriptions=ai_descriptions, model_name=model_name, prompt=None
            )
            logger.info(f"AI ì„¤ëª… {len(saved)}ê±´ ì €ì¥ ì™„ë£Œ (ë¹„ë™ê¸°)")

        return ai_descriptions

    async def _process_single_element_async(
        self,
        async_client: AsyncOpenAI,
        image: np.ndarray,
        element: models.LayoutElement,
        semaphore: asyncio.Semaphore,
        model_name: str,
    ) -> str:
        """
        ë‹¨ì¼ elementì— ëŒ€í•œ ë¹„ë™ê¸° AI ì„¤ëª… ìƒì„± (ì§€ìˆ˜ ë°±ì˜¤í”„ ì¬ì‹œë„ í¬í•¨)

        Args:
            async_client: AsyncOpenAI í´ë¼ì´ì–¸íŠ¸
            image: ì›ë³¸ ì´ë¯¸ì§€
            element: ì²˜ë¦¬í•  ë ˆì´ì•„ì›ƒ ìš”ì†Œ
            semaphore: ë™ì‹œ ìš”ì²­ ìˆ˜ ì œí•œìš© Semaphore
            model_name: ì‚¬ìš©í•  OpenAI ëª¨ë¸ ì´ë¦„

        Returns:
            str: AI ìƒì„± ì„¤ëª… í…ìŠ¤íŠ¸

        ì¬ì‹œë„ ë¡œì§:
        - ìµœëŒ€ 3íšŒ ì¬ì‹œë„
        - ëŒ€ê¸° ì‹œê°„: 1ì´ˆ â†’ 2ì´ˆ â†’ 4ì´ˆ (ì§€ìˆ˜ ë°±ì˜¤í”„)
        """
        # 1. ì´ë¯¸ì§€ í¬ë¡­ ë° ê²€ì¦
        x1, y1 = element.bbox_x, element.bbox_y
        x2, y2 = x1 + element.bbox_width, y1 + element.bbox_height

        # í¬ê¸° ê²€ì¦
        if y2 <= y1 or x2 <= x1:
            logger.warning(f"ìœ íš¨í•˜ì§€ ì•Šì€ BBox í¬ê¸°: Element {element.element_id}")
            return ""

        # ì´ë¯¸ì§€ í¬ë¡­
        cropped_img = image[y1:y2, x1:x2]

        # 2. PIL ì´ë¯¸ì§€ ë³€í™˜ ë° Base64 ì¸ì½”ë”©
        pil_img = Image.fromarray(cv2.cvtColor(cropped_img, cv2.COLOR_BGR2RGB))
        buffered = io.BytesIO()
        pil_img.save(buffered, format="PNG")
        img_base64 = base64.b64encode(buffered.getvalue()).decode("utf-8")

        # 3. í”„ë¡¬í”„íŠ¸ ì„ íƒ
        prompts = {
            "figure": figure_prompt,
            "table": table_prompt,
            "flowchart": flowchart_prompt,
        }
        prompt = prompts.get(element.class_name, f"ì´ {element.class_name} ë‚´ìš© ì„¤ëª…")

        system_prompt = (
            "ë‹¹ì‹ ì€ ì‹œê° ì¥ì•  ì•„ë™ í•™ìŠµ AI ë¹„ì„œì…ë‹ˆë‹¤. "
            "ì‹œê° ìë£Œ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ê°„ê²°, ëª…í™•í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”. "
            "ìŒì„± ë³€í™˜ ê°€ëŠ¥í•˜ê²Œ ì§ì ‘ì ì´ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì‘ì„±í•˜ì„¸ìš”."
        )

        # 4. ì§€ìˆ˜ ë°±ì˜¤í”„ ì¬ì‹œë„ ë¡œì§
        max_retries = 3
        base_delay = 1.0  # ì´ˆ ë‹¨ìœ„

        async with semaphore:  # Rate Limit ì œì–´
            for attempt in range(max_retries):
                try:
                    # API í˜¸ì¶œ
                    response = await async_client.chat.completions.create(
                        model=model_name,
                        messages=[
                            {"role": "system", "content": system_prompt},
                            {
                                "role": "user",
                                "content": [
                                    {"type": "text", "text": prompt},
                                    {
                                        "type": "image_url",
                                        "image_url": {
                                            "url": f"data:image/png;base64,{img_base64}"
                                        },
                                    },
                                ],
                            },
                        ],
                        temperature=0.2,
                        max_tokens=600,
                    )

                    # ì„±ê³µ ì‹œ ê²°ê³¼ ë°˜í™˜
                    description = response.choices[0].message.content.strip()
                    logger.debug(
                        f"API ì‘ë‹µ ì™„ë£Œ (ì‹œë„ {attempt + 1}/{max_retries}): "
                        f"Element {element.element_id}"
                    )
                    return description

                except openai.RateLimitError as e:
                    # Rate Limit ì˜¤ë¥˜: ì§€ìˆ˜ ë°±ì˜¤í”„ ëŒ€ê¸° í›„ ì¬ì‹œë„
                    if attempt < max_retries - 1:
                        delay = base_delay * (2**attempt)  # 1ì´ˆ â†’ 2ì´ˆ â†’ 4ì´ˆ
                        logger.warning(
                            f"âš ï¸ Rate Limit ì˜¤ë¥˜ (Element {element.element_id}): "
                            f"{delay}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„ ({attempt + 1}/{max_retries})"
                        )
                        await asyncio.sleep(delay)
                    else:
                        logger.error(
                            f"âŒ Rate Limit ì˜¤ë¥˜ ìµœì¢… ì‹¤íŒ¨ (Element {element.element_id}): {e}"
                        )
                        raise  # ìµœì¢… ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸ ì „íŒŒ

                except openai.APIError as e:
                    # API ì¼ë°˜ ì˜¤ë¥˜: ì§€ìˆ˜ ë°±ì˜¤í”„ ëŒ€ê¸° í›„ ì¬ì‹œë„
                    if attempt < max_retries - 1:
                        delay = base_delay * (2**attempt)
                        logger.warning(
                            f"âš ï¸ API ì˜¤ë¥˜ (Element {element.element_id}): "
                            f"{delay}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„ ({attempt + 1}/{max_retries}) - {e}"
                        )
                        await asyncio.sleep(delay)
                    else:
                        logger.error(
                            f"âŒ API ì˜¤ë¥˜ ìµœì¢… ì‹¤íŒ¨ (Element {element.element_id}): {e}"
                        )
                        raise

                except Exception as e:
                    # ê¸°íƒ€ ì˜ˆì™¸: ì¦‰ì‹œ ì‹¤íŒ¨
                    logger.error(
                        f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ (Element {element.element_id}): {e}",
                        exc_info=True,
                    )
                    raise

        # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜ (unreachable, but for type safety)
        return ""

    def visualize_results(
        self, image: np.ndarray, layout_elements: List[models.LayoutElement]
    ) -> np.ndarray:
        """ê²°ê³¼ ì‹œê°í™” (ê¸°ì¡´ê³¼ ë™ì¼)"""
        img_result = image.copy()
        overlay = image.copy()
        random.seed(42)
        unique_classes = list({elem.class_name for elem in layout_elements})
        class_colors = {}
        for i, cls_name in enumerate(unique_classes):
            h, s, v = i / max(1, len(unique_classes)), 0.8, 0.9
            r, g, b = colorsys.hsv_to_rgb(h, s, v)
            class_colors[cls_name] = (int(b * 255), int(g * 255), int(r * 255))
        for element in layout_elements:
            x1, y1 = element.bbox_x, element.bbox_y
            x2, y2 = x1 + element.bbox_width, y1 + element.bbox_height
            cls_name, color = element.class_name, class_colors[element.class_name]
            cv2.rectangle(overlay, (x1, y1), (x2, y2), color, -1)
            cv2.rectangle(img_result, (x1, y1), (x2, y2), color, 2)
            label = f"{cls_name} ({element.confidence:.2f})"
            labelSize, _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)
            y1_label = max(y1, labelSize[1] + 10)
            cv2.rectangle(
                img_result,
                (x1, y1_label - labelSize[1] - 10),
                (x1 + labelSize[0], y1_label),
                color,
                -1,
            )
            cv2.putText(
                img_result,
                label,
                (x1, y1_label - 5),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.5,
                (255, 255, 255),
                1,
            )
        img_result = cv2.addWeighted(overlay, 0.2, img_result, 0.8, 0)
        return cv2.cvtColor(img_result, cv2.COLOR_BGR2RGB)


def analyze_page(
    *,
    page_id: int,
    image: np.ndarray,
    db: Session,
    api_key: Optional[str] = None,
    model_choice: Optional[str] = None,
) -> Dict[str, object]:
    """ë‹¨ì¼ í˜ì´ì§€ì— ëŒ€í•œ ì „ì²´ ë¶„ì„ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•œë‹¤."""
    service = AnalysisService(
        model_choice=model_choice or "SmartEyeSsen", auto_load=False
    )

    layout_elements = service.analyze_layout(
        image=image, page_id=page_id, db=db, model_choice=model_choice
    )

    text_contents = service.perform_ocr(
        image=image, layout_elements=layout_elements, db=db
    )

    ai_descriptions: Dict[int, str] = {}
    if api_key:
        ai_descriptions = service.call_openai_api(
            image=image, layout_elements=layout_elements, api_key=api_key, db=db
        )

    return {
        "layout_elements": layout_elements,
        "text_contents": text_contents,
        "ai_descriptions": ai_descriptions,
    }
