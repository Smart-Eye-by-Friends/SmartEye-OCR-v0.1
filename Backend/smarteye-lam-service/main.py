# -*- coding: utf-8 -*-
"""
SmartEye LAM (Layout Analysis Module) ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤
ë ˆì´ì•„ì›ƒ ë¶„ì„ ì „ìš© ë…ë¦½ ì„œë¹„ìŠ¤
"""

import os
import sys
import time
import tempfile
import traceback
from typing import Dict, List, Optional

# FastAPI ê´€ë ¨
from fastapi import FastAPI, File, Form, UploadFile, HTTPException
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware

# AI/ML ê´€ë ¨
import torch
from huggingface_hub import hf_hub_download

# ë¡œê¹…
from loguru import logger

# êµ¬ì¡°í™”ëœ ë¶„ì„ ê¸°ëŠ¥ì€ Java ë°±ì—”ë“œì—ì„œ ì²˜ë¦¬

# ë¡œê·¸ ì„¤ì •
logger.remove()
logger.add(sys.stderr, level="INFO", format="{time} | {level} | {module}:{function}:{line} - {message}")

# ë””ë°”ì´ìŠ¤ ì„¤ì •
device = 'cuda' if torch.cuda.is_available() else 'cpu'

print("ğŸš€ SmartEye LAM ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
print("ğŸ“± ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:8001 ìœ¼ë¡œ ì ‘ì†í•˜ì„¸ìš”")
print("ğŸ“š API ë¬¸ì„œëŠ” http://localhost:8001/docs ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
print(f"ğŸ–¥ï¸ ë””ë°”ì´ìŠ¤: {device}")

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="SmartEye LAM Service",
    description="Layout Analysis Module - ë ˆì´ì•„ì›ƒ ë¶„ì„ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤",
    version="1.0.0"
)

# CORS ì„¤ì • - ë³´ì•ˆ ê°•í™”
import os
allowed_origins = os.getenv("CORS_ALLOWED_ORIGINS", "http://localhost:3000,http://localhost:8080").split(",")

app.add_middleware(
    CORSMiddleware,
    allow_origins=allowed_origins,
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["Content-Type", "Authorization", "X-Requested-With"],
)

class LAMAnalyzer:
    def __init__(self):
        self.models_cache = {}
        self.device = device
        self.model = None
        
        # ëª¨ë¸ ì„¤ì •
        self.model_configs = {
            "SmartEyeSsen": {"imgsz": 1024, "conf": 0.25, "description": "SmartEye ì „ìš© ëª¨ë¸"},
            "docsynth300k": {"imgsz": 1600, "conf": 0.20, "description": "DocLayout-YOLO DocSynth300K"},
            "doclaynet_docsynth": {"imgsz": 1024, "conf": 0.25, "description": "DocLayout-YOLO DocLayNet"},
            "docstructbench": {"imgsz": 1024, "conf": 0.25, "description": "DocLayout-YOLO DocStructBench"}
        }

    def download_model(self, model_choice="SmartEyeSsen"):
        """HuggingFace Hubì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
        models = {
            "SmartEyeSsen": {
                "repo_id": "AkJeond/SmartEyeSsen",
                "filename": "best_tuned_model.pt"
            },
            "doclaynet_docsynth": {
                "repo_id": "juliozhao/DocLayout-YOLO-DocLayNet-Docsynth300K_pretrained",
                "filename": "doclayout_yolo_doclaynet_imgsz1120_docsynth_pretrain.pt"
            },
            "docsynth300k": {
                "repo_id": "juliozhao/DocLayout-YOLO-DocSynth300K-pretrain",
                "filename": "doclayout_yolo_docsynth300k_imgsz1600.pt"
            },
            "docstructbench": {
                "repo_id": "juliozhao/DocLayout-YOLO-DocStructBench",
                "filename": "doclayout_yolo_docstructbench_imgsz1024.pt"
            }
        }
        
        selected_model = models.get(model_choice)
        if not selected_model:
            logger.error(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸: {model_choice}")
            return None
            
        try:
            logger.info(f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘: {selected_model['repo_id']}")
            
            model_path = hf_hub_download(
                repo_id=selected_model["repo_id"],
                filename=selected_model["filename"],
                cache_dir="./models",
                force_download=False,
                resume_download=True
            )
            
            logger.info(f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {model_path}")
            return model_path
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ - {model_choice}: {str(e)}")
            return None

    def load_model(self, model_choice="SmartEyeSsen"):
        """ëª¨ë¸ ë¡œë“œ (ê°•í™”ëœ í˜¸í™˜ì„± ë° í´ë°±)"""
        try:
            # ìºì‹œëœ ëª¨ë¸ í™•ì¸
            if model_choice in self.models_cache:
                cached_data = self.models_cache[model_choice]
                if isinstance(cached_data, dict):
                    self.model = cached_data["model"]
                    model_type = cached_data.get("type", "Unknown")
                else:
                    # ì´ì „ ë²„ì „ í˜¸í™˜ì„±
                    self.model = cached_data
                    model_type = "Legacy"
                logger.info(f"ìºì‹œëœ ëª¨ë¸ ì‚¬ìš©: {model_choice} ({model_type})")
                return True

            # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
            model_path = self.download_model(model_choice)
            if not model_path:
                logger.error(f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {model_choice}")
                return False

            logger.info(f"ëª¨ë¸ ë¡œë“œ ì¤‘: {model_choice}")
            
            # 1ì°¨ ì‹œë„: DocLayout-YOLO
            model = None
            model_type = "Unknown"
            
            try:
                from doclayout_yolo import YOLOv10
                model = YOLOv10(model_path)
                model_type = "DocLayout-YOLO"
                logger.info(f"âœ… DocLayout-YOLO ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_choice}")
                
                # ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ìœ¼ë¡œ í˜¸í™˜ì„± í™•ì¸
                import tempfile
                import numpy as np
                from PIL import Image
                
                # ë”ë¯¸ ì´ë¯¸ì§€ë¡œ í…ŒìŠ¤íŠ¸
                test_img = Image.fromarray(np.zeros((640, 640, 3), dtype=np.uint8))
                with tempfile.NamedTemporaryFile(suffix='.jpg', delete=False) as tmp:
                    test_img.save(tmp.name)
                    test_results = model.predict(tmp.name, verbose=False, save=False)
                    import os
                    os.unlink(tmp.name)
                
                logger.info(f"âœ… DocLayout-YOLO í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ í†µê³¼: {model_choice}")
                
            except Exception as e:
                logger.warning(f"DocLayout-YOLO ì‹¤íŒ¨: {e}")
                model = None
                
                # 2ì°¨ ì‹œë„: Ultralytics YOLO í´ë°±
                try:
                    from ultralytics import YOLO
                    model = YOLO(model_path)
                    model_type = "Ultralytics-YOLO"
                    logger.info(f"âš ï¸ Ultralytics YOLOë¡œ í´ë°±: {model_choice}")
                    
                except Exception as e2:
                    logger.error(f"âŒ Ultralytics YOLOë„ ì‹¤íŒ¨: {e2}")
                    return False
            
            if model is None:
                logger.error(f"âŒ ëª¨ë“  YOLO ë¡œë“œ ì‹œë„ ì‹¤íŒ¨: {model_choice}")
                return False
            
            # ëª¨ë¸ ì„¤ì • (ì¶”ë¡  ëª¨ë“œë§Œ)
            if hasattr(model, 'to'):
                model.to(self.device)
            # eval() í˜¸ì¶œ ì‹œ í›ˆë ¨ ê´€ë ¨ ì„¤ì •ì´ ë¡œë“œë˜ì–´ ì˜¤ë¥˜ ë°œìƒí•˜ë¯€ë¡œ ìƒëµ
            
            # ìºì‹œì— ì €ì¥ (ëª¨ë¸ íƒ€ì… ì •ë³´ë„ í•¨ê»˜)
            self.models_cache[model_choice] = {"model": model, "type": model_type}
            self.model = model
            
            logger.info(f"âœ… ëª¨ë¸ ë¡œë“œ ë° ìºì‹œ ì™„ë£Œ: {model_choice} ({model_type})")
            return True
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return False

    def analyze_layout(self, image_path: str, model_choice: str = "SmartEyeSsen"):
        """ë ˆì´ì•„ì›ƒ ë¶„ì„ ìˆ˜í–‰"""
        try:
            # ëª¨ë¸ ë¡œë“œ
            if not self.load_model(model_choice):
                return None
            
            # ëª¨ë¸ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
            config = self.model_configs.get(model_choice, self.model_configs["SmartEyeSsen"])
            
            logger.info(f"ë¶„ì„ ì‹œì‘ - ì´ë¯¸ì§€: {image_path}, ëª¨ë¸: {model_choice}")
            logger.info(f"ì„¤ì • - imgsz: {config['imgsz']}, conf: {config['conf']}")
            
            # ì˜ˆì¸¡ ìˆ˜í–‰
            results = self.model.predict(
                image_path,
                imgsz=config["imgsz"],
                conf=config["conf"],
                iou=0.45,
                device=self.device,
                verbose=False,
                save=False
            )
            
            # ê²°ê³¼ ì²˜ë¦¬
            if not results:
                logger.warning("ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            result = results[0]
            
            # ë°”ìš´ë”© ë°•ìŠ¤ ì •ë³´ ì¶”ì¶œ
            layout_info = []
            
            if hasattr(result, 'boxes') and result.boxes is not None:
                boxes = result.boxes.xyxy.cpu().numpy()
                scores = result.boxes.conf.cpu().numpy()
                classes = result.boxes.cls.cpu().numpy()
                
                # í´ë˜ìŠ¤ ì´ë¦„ ë§¤í•‘
                class_names = getattr(result, 'names', {})
                
                logger.info(f"ê°ì§€ëœ ê°ì²´ ìˆ˜: {len(boxes)}")
                logger.info(f"í´ë˜ìŠ¤ ë¶„í¬: {dict(zip(*torch.unique(result.boxes.cls, return_counts=True)))}")
                
                for i, (box, score, cls_id) in enumerate(zip(boxes, scores, classes)):
                    x1, y1, x2, y2 = box
                    class_name = class_names.get(int(cls_id), f"class_{int(cls_id)}")
                    
                    layout_info.append({
                        "class": class_name,
                        "class_id": int(cls_id),
                        "confidence": float(score),
                        "bbox": {
                            "x1": float(x1),
                            "y1": float(y1),
                            "x2": float(x2),
                            "y2": float(y2)
                        }
                    })
            
            logger.info(f"ë¶„ì„ ì™„ë£Œ - ì´ {len(layout_info)}ê°œ ìš”ì†Œ ê°ì§€")
            return layout_info
            
        except Exception as e:
            logger.error(f"ë ˆì´ì•„ì›ƒ ë¶„ì„ ì‹¤íŒ¨: {e}")
            logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return None

# Level 3: LAM ì„œë¹„ìŠ¤ ë©”ëª¨ë¦¬ ìµœì í™” ë²„ì „
class MemoryOptimizedLAMAnalyzer(LAMAnalyzer):
    """
    ë©”ëª¨ë¦¬ ìµœì í™”ëœ LAM ë¶„ì„ê¸°
    - ëª¨ë¸ ì–¸ë¡œë“œ ê¸°ëŠ¥
    - CUDA ë©”ëª¨ë¦¬ ìë™ ì •ë¦¬
    - ë°°ì¹˜ ì²˜ë¦¬ ì§€ì›
    """
    
    def __init__(self):
        super().__init__()
        self.memory_stats = {
            "peak_memory_mb": 0,
            "current_memory_mb": 0,
            "model_loads": 0,
            "model_unloads": 0,
            "cache_hits": 0,
            "gc_calls": 0
        }
        
    def get_memory_usage(self):
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë°˜í™˜ (MB ë‹¨ìœ„)"""
        import psutil
        import gc
        
        process = psutil.Process()
        memory_mb = process.memory_info().rss / 1024 / 1024
        
        # CUDA ë©”ëª¨ë¦¬ í™•ì¸ (ê°€ëŠ¥í•œ ê²½ìš°)
        cuda_memory_mb = 0
        if torch.cuda.is_available():
            cuda_memory_mb = torch.cuda.memory_allocated() / 1024 / 1024
            
        total_memory_mb = memory_mb + cuda_memory_mb
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self.memory_stats["current_memory_mb"] = total_memory_mb
        if total_memory_mb > self.memory_stats["peak_memory_mb"]:
            self.memory_stats["peak_memory_mb"] = total_memory_mb
            
        return {
            "ram_mb": round(memory_mb, 2),
            "cuda_mb": round(cuda_memory_mb, 2),
            "total_mb": round(total_memory_mb, 2)
        }
    
    def cleanup_memory(self, force_gc=True):
        """ë©”ëª¨ë¦¬ ì •ë¦¬ ë° CUDA ìºì‹œ í•´ì œ"""
        logger.info("ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œì‘...")
        
        # CUDA ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            logger.info("CUDA ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
        
        # Python ê°€ë¹„ì§€ ì»¨ë ‰í„°
        if force_gc:
            import gc
            collected = gc.collect()
            self.memory_stats["gc_calls"] += 1
            logger.info(f"ê°€ë¹„ì§€ ì»¨ë ‰í„° ì‹¤í–‰: {collected}ê°œ ê°ì²´ í•´ì œ")
    
    def unload_model(self, model_choice):
        """ëª¨ë¸ ì–¸ë¡œë“œ ë° ë©”ëª¨ë¦¬ í•´ì œ"""
        if model_choice in self.models_cache:
            logger.info(f"ëª¨ë¸ ì–¸ë¡œë“œ: {model_choice}")
            
            # ëª¨ë¸ ì°¸ì¡° ì‚­ì œ
            del self.models_cache[model_choice]
            
            # í˜„ì¬ ëª¨ë¸ì´ ì–¸ë¡œë“œëœ ëª¨ë¸ì´ë©´ Noneìœ¼ë¡œ ì„¤ì •
            if self.model is not None:
                self.model = None
                
            self.memory_stats["model_unloads"] += 1
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            self.cleanup_memory()
            
            logger.info(f"ëª¨ë¸ {model_choice} ì–¸ë¡œë“œ ì™„ë£Œ")
            return True
        return False
    
    def load_model(self, model_choice="SmartEyeSsen"):
        """ë©”ëª¨ë¦¬ ìµœì í™”ëœ ëª¨ë¸ ë¡œë“œ"""
        try:
            # ìºì‹œ íˆíŠ¸ í™•ì¸
            if model_choice in self.models_cache:
                self.memory_stats["cache_hits"] += 1
                logger.info(f"ìºì‹œëœ ëª¨ë¸ ì‚¬ìš©: {model_choice}")
                
                cached_data = self.models_cache[model_choice]
                if isinstance(cached_data, dict):
                    self.model = cached_data["model"]
                else:
                    self.model = cached_data
                return True
            
            # ê¸°ì¡´ ëª¨ë¸ì´ ë‹¤ë¥´ë©´ ì–¸ë¡œë“œ (ë©”ëª¨ë¦¬ ì ˆì•½)
            if self.model is not None and len(self.models_cache) > 0:
                # ìµœëŒ€ 2ê°œ ëª¨ë¸ë§Œ ìœ ì§€ (ë©”ëª¨ë¦¬ ì œí•œ)
                if len(self.models_cache) >= 2:
                    # LRU: ê°€ì¥ ì˜¤ë˜ëœ ëª¨ë¸ ì–¸ë¡œë“œ
                    oldest_model = next(iter(self.models_cache))
                    self.unload_model(oldest_model)
                    logger.info(f"LRU ì •ì±…ìœ¼ë¡œ ëª¨ë¸ ì–¸ë¡œë“œ: {oldest_model}")
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¡œê¹… (ë¡œë“œ ì „)
            memory_before = self.get_memory_usage()
            logger.info(f"ëª¨ë¸ ë¡œë“œ ì „ ë©”ëª¨ë¦¬: {memory_before}")
            
            # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
            model_path = self.download_model(model_choice)
            if not model_path:
                logger.error(f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {model_choice}")
                return False
            
            # ëª¨ë¸ ë¡œë“œ
            success = super().load_model(model_choice)
            if success:
                self.memory_stats["model_loads"] += 1
                
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¡œê¹… (ë¡œë“œ í›„)
                memory_after = self.get_memory_usage()
                memory_diff = memory_after["total_mb"] - memory_before["total_mb"]
                logger.info(f"ëª¨ë¸ ë¡œë“œ í›„ ë©”ëª¨ë¦¬: {memory_after} (+{memory_diff:.2f}MB)")
            
            return success
            
        except Exception as e:
            logger.error(f"ë©”ëª¨ë¦¬ ìµœì í™”ëœ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def analyze_layout(self, image_path: str, model_choice: str = "SmartEyeSsen"):
        """ë©”ëª¨ë¦¬ ìµœì í™”ëœ ë ˆì´ì•„ì›ƒ ë¶„ì„"""
        try:
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ ì‹œì‘
            memory_start = self.get_memory_usage()
            logger.info(f"ë¶„ì„ ì‹œì‘ ë©”ëª¨ë¦¬: {memory_start}")
            
            # ëª¨ë¸ ë¡œë“œ
            if not self.load_model(model_choice):
                return None
            
            # ë¶„ì„ ìˆ˜í–‰
            results = super().analyze_layout(image_path, model_choice)
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ
            memory_end = self.get_memory_usage()
            memory_diff = memory_end["total_mb"] - memory_start["total_mb"]
            logger.info(f"ë¶„ì„ ì™„ë£¼ ë©”ëª¨ë¦¬: {memory_end} (+{memory_diff:.2f}MB)")
            
            # ì„ê³„ê°’ ì´ˆê³¼ ì‹œ ë©”ëª¨ë¦¬ ì •ë¦¬ (ì˜ˆ: 4GB)
            if memory_end["total_mb"] > 4096:
                logger.warning(f"ë©”ëª¨ë¦¬ ì„ê³„ê°’ ì´ˆê³¼ ({memory_end['total_mb']:.2f}MB > 4096MB), ì •ë¦¬ ì‹œì‘")
                self.cleanup_memory(force_gc=True)
                
                memory_after_cleanup = self.get_memory_usage()
                logger.info(f"ì •ë¦¬ í›„ ë©”ëª¨ë¦¬: {memory_after_cleanup}")
            
            return results
            
        except Exception as e:
            logger.error(f"ë©”ëª¨ë¦¬ ìµœì í™” ë¶„ì„ ì‹¤íŒ¨: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë©”ëª¨ë¦¬ ì •ë¦¬
            self.cleanup_memory(force_gc=True)
            return None
    
    def get_memory_stats(self):
        """ë©”ëª¨ë¦¬ í†µê³„ ë°˜í™˜"""
        current_memory = self.get_memory_usage()
        stats = self.memory_stats.copy()
        stats["current_memory"] = current_memory
        stats["cached_models"] = list(self.models_cache.keys())
        stats["cache_size"] = len(self.models_cache)
        return stats

# ë©”ëª¨ë¦¬ ìµœì í™”ëœ ê¸€ë¡œë²Œ ë¶„ì„ê¸° ì¸ìŠ¤í„´ìŠ¤
analyzer = MemoryOptimizedLAMAnalyzer()

@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {"message": "SmartEye LAM Service", "status": "running", "device": device}

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸ + ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì •ë³´"""
    memory_info = analyzer.get_memory_usage()
    memory_stats = analyzer.get_memory_stats()
    
    return {
        "status": "healthy", 
        "device": device, 
        "cached_models": list(analyzer.models_cache.keys()),
        "memory_usage": memory_info,
        "memory_stats": memory_stats
    }

@app.post("/analyze-layout")
async def analyze_layout(
    image: UploadFile = File(...),
    model_choice: str = Form("SmartEyeSsen")
):
    """ë ˆì´ì•„ì›ƒ ë¶„ì„ ë©”ì¸ ì—”ë“œí¬ì¸íŠ¸"""
    start_time = time.time()
    
    try:
        logger.info(f"ë ˆì´ì•„ì›ƒ ë¶„ì„ ìš”ì²­ - íŒŒì¼: {image.filename}, ëª¨ë¸: {model_choice}")
        
        # ì§€ì› ëª¨ë¸ í™•ì¸
        if model_choice not in analyzer.model_configs:
            raise HTTPException(
                status_code=400, 
                detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸: {model_choice}. ì§€ì› ëª¨ë¸: {list(analyzer.model_configs.keys())}"
            )
        
        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        with tempfile.NamedTemporaryFile(suffix='.jpg', delete=False) as temp_file:
            content = await image.read()
            temp_file.write(content)
            temp_file_path = temp_file.name
        
        try:
            # ë ˆì´ì•„ì›ƒ ë¶„ì„ ìˆ˜í–‰
            layout_results = analyzer.analyze_layout(temp_file_path, model_choice)
            
            if layout_results is None:
                raise HTTPException(status_code=500, detail="ëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            
            processing_time = time.time() - start_time
            
            response = {
                "success": True,
                "processing_time": round(processing_time, 2),
                "model_used": model_choice,
                "device": device,
                "results": {
                    "layout_analysis": layout_results,
                    "total_elements": len(layout_results)
                }
            }
            
            logger.info(f"ë¶„ì„ ì™„ë£Œ - ì²˜ë¦¬ì‹œê°„: {processing_time:.2f}ì´ˆ, ìš”ì†Œ ìˆ˜: {len(layout_results)}")
            return JSONResponse(content=response)
            
        finally:
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            os.unlink(temp_file_path)
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"ì„œë²„ ì˜¤ë¥˜: {str(e)}")

# Level 3: ë©”ëª¨ë¦¬ ê´€ë¦¬ ì—”ë“œí¬ì¸íŠ¸ë“¤

@app.post("/memory/cleanup")
async def cleanup_memory():
    """ë‹¤ì´ë ‰íŠ¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ìš”ì²­"""
    try:
        memory_before = analyzer.get_memory_usage()
        analyzer.cleanup_memory(force_gc=True)
        memory_after = analyzer.get_memory_usage()
        
        memory_freed = memory_before["total_mb"] - memory_after["total_mb"]
        
        return {
            "success": True,
            "memory_before": memory_before,
            "memory_after": memory_after,
            "memory_freed_mb": round(memory_freed, 2),
            "message": f"{memory_freed:.2f}MB ë©”ëª¨ë¦¬ í•´ì œ"
        }
    except Exception as e:
        logger.error(f"ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
        return {"success": False, "error": str(e)}

@app.post("/memory/unload-model")
async def unload_model(model_name: str):
    """íŠ¹ì • ëª¨ë¸ ì–¸ë¡œë“œ ìš”ì²­"""
    try:
        memory_before = analyzer.get_memory_usage()
        success = analyzer.unload_model(model_name)
        
        if success:
            memory_after = analyzer.get_memory_usage()
            memory_freed = memory_before["total_mb"] - memory_after["total_mb"]
            
            return {
                "success": True,
                "model_name": model_name,
                "memory_before": memory_before,
                "memory_after": memory_after,
                "memory_freed_mb": round(memory_freed, 2),
                "message": f"ëª¨ë¸ {model_name} ì–¸ë¡œë“œ ì™„ë£Œ"
            }
        else:
            return {
                "success": False,
                "model_name": model_name,
                "message": f"ëª¨ë¸ {model_name}ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
            }
            
    except Exception as e:
        logger.error(f"ëª¨ë¸ ì–¸ë¡œë“œ ì‹¤íŒ¨: {e}")
        return {"success": False, "error": str(e)}

@app.get("/memory/stats")
async def get_memory_stats():
    """ìƒì„¸ ë©”ëª¨ë¦¬ í†µê³„ ì¡°íšŒ"""
    try:
        return {
            "success": True,
            "stats": analyzer.get_memory_stats(),
            "device_info": {
                "device": device,
                "cuda_available": torch.cuda.is_available(),
                "cuda_device_count": torch.cuda.device_count() if torch.cuda.is_available() else 0
            }
        }
    except Exception as e:
        logger.error(f"ë©”ëª¨ë¦¬ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {"success": False, "error": str(e)}

# /analyze-structured ì—”ë“œí¬ì¸íŠ¸ëŠ” ì œê±°ë¨ - êµ¬ì¡°í™” ë¶„ì„ì€ Java ë°±ì—”ë“œì—ì„œ ì²˜ë¦¬

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001, reload=True)
